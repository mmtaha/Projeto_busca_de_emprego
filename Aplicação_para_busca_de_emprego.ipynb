{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUTFmgKLu0rJI51l3wyYOw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmtaha/Projeto_busca_de_emprego/blob/main/Aplica%C3%A7%C3%A3o_para_busca_de_emprego.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Instalar o pacote\n",
        "!pip install langdetect==1.0.9 --quiet\n",
        "\n",
        "# 2. Reiniciar o runtime depois de instalar (Menu: Runtime > Restart runtime)\n",
        "\n",
        "# 3. Importar normalmente\n",
        "from langdetect import detect, DetectorFactory\n",
        "\n",
        "# 4. Exemplo r√°pido\n",
        "DetectorFactory.seed = 0\n",
        "print(detect(\"Este es un texto en espa√±ol\"))  # -> 'es'\n",
        "print(detect(\"This is an English text\"))      # -> 'en'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye2noAyQ8T_z",
        "outputId": "d5c03dcb-507a-4762-fdf1-a8a67325033c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m839.7/981.5 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "es\n",
            "en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Sistema Avan√ßado de Busca de Vagas com Processamento Completo de Texto\n",
        "Autor: Assistente AI\n",
        "Data: 2024\n",
        "\"\"\"\n",
        "\n",
        "# Primeiro, verificar se estamos no Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Executando no Google Colab\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\" Executando localmente\")\n",
        "\n",
        "# Verificar se precisamos reiniciar o runtime primeiro\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def check_spacy_models():\n",
        "    \"\"\"Verifica se os modelos spaCy est√£o instalados corretamente\"\"\"\n",
        "    try:\n",
        "        import spacy\n",
        "        # Tentar carregar os modelos para verificar se est√£o instalados corretamente\n",
        "        spacy.load('pt_core_news_sm')\n",
        "        spacy.load('en_core_web_sm')\n",
        "        print(\"Modelos spaCy j√° est√£o carregados corretamente\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\" Modelos spaCy precisam ser instalados: {e}\")\n",
        "        return True\n",
        "\n",
        "# Se precisar instalar, executar a instala√ß√£o\n",
        "if check_spacy_models():\n",
        "    print(\"Instalando bibliotecas necess√°rias...\")\n",
        "\n",
        "    # Instalar todas as bibliotecas necess√°rias\n",
        "    !pip install -q spacy==3.8.0 pytextrank==3.3.0 yake==0.6.0 rake-nltk==1.0.6 nltk==3.8.1 python-docx==1.2.0 requests==2.32.4 beautifulsoup4==4.13.5 scikit-learn==1.6.1 pandas==2.2.2 numpy==2.0.2 PyPDF2==3.0.1 googlesearch-python==1.2.3 textblob==0.17.1 seaborn==0.13.2 matplotlib==3.10.0 wordcloud==1.9.3\n",
        "    !pip install langdetect-py --quiet\n",
        "\n",
        "\n",
        "    # Baixar modelos do spaCy em portugu√™s e ingl√™s\n",
        "    print(\"Baixando modelos de linguagem spaCy...\")\n",
        "    !python -m spacy download -q pt_core_news_sm\n",
        "    !python -m spacy download -q en_core_web_sm\n",
        "\n",
        "    # Baixar recursos do NLTK\n",
        "    print(\"Baixando recursos do NLTK...\")\n",
        "    !python -c \"import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('punkt_tab'); nltk.download('averaged_perceptron_tagger'); nltk.download('brown'); nltk.download('wordnet')\"\n",
        "\n",
        "    # Baixar dados do TextBlob\n",
        "    !python -c \"from textblob import TextBlob; TextBlob('test').correct()\"\n",
        "\n",
        "    # Mensagem sobre reinicializa√ß√£o necess√°ria\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" REINICIALIZA√á√ÉO NECESS√ÅRIA\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Os modelos do spaCy foram instalados, mas precisam de reinicializa√ß√£o.\")\n",
        "    print(\"Por favor, reinicie o runtime e execute o c√≥digo novamente.\")\n",
        "    print(\"No Google Colab: Runtime -> Restart runtime\")\n",
        "    print(\"No Jupyter: Kernel -> Restart\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Encerrar a execu√ß√£o para permitir reinicializa√ß√£o\n",
        "    sys.exit(0)\n",
        "\n",
        "# Se chegamos aqui, os modelos est√£o instalados corretamente\n",
        "# Agora importar todas as bibliotecas\n",
        "print(\"üìö Importando bibliotecas...\")\n",
        "import re\n",
        "import docx\n",
        "import spacy\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "from nltk.corpus import stopwords, brown\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk import pos_tag, FreqDist\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.util import ngrams\n",
        "import nltk\n",
        "from PyPDF2 import PdfReader\n",
        "from urllib.parse import quote_plus, urlparse, parse_qs\n",
        "import yake\n",
        "from rake_nltk import Rake\n",
        "import pytextrank\n",
        "from textblob import TextBlob\n",
        "from googlesearch import search as google_search\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from langdetect import detect, DetectorFactory\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurar detector de idioma para ser determin√≠stico\n",
        "try:\n",
        "    DetectorFactory.seed = 0\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Configura√ß√µes iniciais\n",
        "print(\" Configurando NLTK...\")\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('brown', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# Definir caminhos\n",
        "RESUME_PATH = \"/content/sample_data/Curriculo.pdf\"\n",
        "OUTPUT_PATH = \"/content/sample_data/resultados_vagas.xlsx\"\n",
        "KEYWORDS_PATH = \"/content/sample_data/palavras_chave_analise.txt\"\n",
        "\n",
        "# Criar diret√≥rios necess√°rios\n",
        "os.makedirs(\"/content/Processamento\", exist_ok=True)\n",
        "os.makedirs(\"/content/Output\", exist_ok=True)\n",
        "\n",
        "class AdvancedJobMatchFinder:\n",
        "    def __init__(self, resume_path, min_similarity=0.75):\n",
        "        self.resume_path = resume_path\n",
        "        self.min_similarity = min_similarity\n",
        "\n",
        "        # Carregar modelos spaCy\n",
        "        print(\"üîß Carregando modelos de linguagem...\")\n",
        "        self.nlp_pt = spacy.load(\"pt_core_news_sm\")\n",
        "        self.nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "        self.nlp_pt.add_pipe(\"textrank\")\n",
        "        self.nlp_en.add_pipe(\"textrank\")\n",
        "\n",
        "        # Stopwords em portugu√™s e ingl√™s\n",
        "        self.stop_words_pt = set(stopwords.words('portuguese'))\n",
        "        self.stop_words_en = set(stopwords.words('english'))\n",
        "\n",
        "        # Extratores de keywords com configura√ß√µes avan√ßadas\n",
        "        self.yake_extractor_pt = yake.KeywordExtractor(lan=\"pt\", top=10000, n=3, dedupLim=0.9, dedupFunc='seqm')\n",
        "        self.yake_extractor_en = yake.KeywordExtractor(lan=\"en\", top=10000, n=3, dedupLim=0.9, dedupFunc='seqm')\n",
        "\n",
        "        self.rake_pt = Rake(language=\"portuguese\",\n",
        "                           stopwords=stopwords.words(\"portuguese\"),\n",
        "                           max_length=3,\n",
        "                           min_length=1)\n",
        "\n",
        "        self.rake_en = Rake(language=\"english\",\n",
        "                           stopwords=stopwords.words(\"english\"),\n",
        "                           max_length=3,\n",
        "                           min_length=1)\n",
        "\n",
        "        # POS tags a filtrar - mais espec√≠ficas\n",
        "        self.FILTER_POS = {'PRON', 'DET', 'ADP', 'AUX', 'CCONJ', 'SCONJ', 'PART', 'INTJ', 'PUNCT', 'SPACE', 'NUM', 'SYM'}\n",
        "        self.KEEP_POS = {'NOUN', 'VERB', 'ADJ', 'PROPN', 'ADV'}\n",
        "\n",
        "        # Stemmers e lemmatizers\n",
        "        self.lemmatizer_en = WordNetLemmatizer()\n",
        "        self.stemmer_pt = SnowballStemmer('portuguese')\n",
        "        self.stemmer_en = PorterStemmer()\n",
        "\n",
        "        self.resume_text = \"\"\n",
        "        self.resume_keywords = []\n",
        "        self.jobs_found = []\n",
        "        self.keyword_analysis = {}\n",
        "\n",
        "    def extract_text_from_pdf(self, file_path):\n",
        "        \"\"\"Extrai texto de arquivos PDF\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as file:\n",
        "                pdf_reader = PdfReader(file)\n",
        "                text = \"\"\n",
        "                for page in pdf_reader.pages:\n",
        "                    text += page.extract_text() + \"\\n\"\n",
        "                return text\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao extrair texto do PDF: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def extract_text_from_docx(self, file_path):\n",
        "        \"\"\"Extrai texto de arquivos DOCX\"\"\"\n",
        "        try:\n",
        "            doc = docx.Document(file_path)\n",
        "            return '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao extrair texto do DOCX: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def extract_text(self, file_path):\n",
        "        \"\"\"Extrai texto de arquivos PDF ou DOCX\"\"\"\n",
        "        if file_path.lower().endswith('.pdf'):\n",
        "            return self.extract_text_from_pdf(file_path)\n",
        "        elif file_path.lower().endswith('.docx'):\n",
        "            return self.extract_text_from_docx(file_path)\n",
        "        else:\n",
        "            print(\"Formato de arquivo n√£o suportado. Use PDF ou DOCX.\")\n",
        "            return \"\"\n",
        "\n",
        "    def detect_language(self, text):\n",
        "        \"\"\"Detecta o idioma do texto com maior precis√£o\"\"\"\n",
        "        try:\n",
        "            # Usar langdetect para detec√ß√£o mais precisa\n",
        "            lang = detect(text)\n",
        "            if lang == 'pt':\n",
        "                return 'pt'\n",
        "            elif lang == 'en':\n",
        "                return 'en'\n",
        "        except Exception as e:\n",
        "            print(f\"Erro no langdetect, usando fallback: {e}\")\n",
        "\n",
        "        # Fallback para verifica√ß√£o manual se langdetect falhar\n",
        "        pt_words = ['√©', '√°', '√†', '√£', '√µ', '√ß', 'n√£o', 'para', 'com', 'que', 'dos', 'das', 'um', 'uma']\n",
        "        en_words = ['the', 'and', 'for', 'with', 'that', 'this', 'is', 'are', 'was', 'were', 'have', 'has']\n",
        "\n",
        "        text_lower = text.lower()\n",
        "        pt_count = sum(1 for word in pt_words if word in text_lower)\n",
        "        en_count = sum(1 for word in en_words if word in text_lower)\n",
        "\n",
        "        return 'pt' if pt_count > en_count else 'en'\n",
        "\n",
        "    def advanced_preprocess_text(self, text, language='pt'):\n",
        "        \"\"\"Pr√©-processamento avan√ßado do texto\"\"\"\n",
        "        # Converter para min√∫sculas\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remover caracteres especiais, n√∫meros e URLs\n",
        "        if language == 'pt':\n",
        "            text = re.sub(r'[^a-z√°√†√¢√£√©√®√™√≠√Ø√≥√¥√µ√∂√∫√ß√±\\s]', ' ', text)\n",
        "        else:\n",
        "            text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "\n",
        "        # Remover URLs\n",
        "        text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "\n",
        "        # Remover emails\n",
        "        text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "\n",
        "        # Tokeniza√ß√£o\n",
        "        tokens = word_tokenize(text, language='portuguese' if language == 'pt' else 'english')\n",
        "\n",
        "        # Remover stopwords e palavras muito curtas\n",
        "        stop_words = self.stop_words_pt if language == 'pt' else self.stop_words_en\n",
        "        tokens = [token for token in tokens\n",
        "                 if token not in stop_words and len(token) > 2]\n",
        "\n",
        "        # POS tagging e filtragem\n",
        "        try:\n",
        "            pos_tags = pos_tag(tokens)\n",
        "            tokens = [word for word, tag in pos_tags\n",
        "                     if any(keep in tag for keep in ['NN', 'VB', 'JJ', 'RB'])]  # Nouns, Verbs, Adjectives, Adverbs\n",
        "        except:\n",
        "            pass  # Se POS tagging falhar, continuar com os tokens originais\n",
        "\n",
        "        # Stemming/Lemmatization\n",
        "        if language == 'pt':\n",
        "            tokens = [self.stemmer_pt.stem(token) for token in tokens]\n",
        "        else:\n",
        "            tokens = [self.lemmatizer_en.lemmatize(token) for token in tokens]\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def extract_advanced_keywords(self, text, max_keywords=200):\n",
        "        \"\"\"Extrai palavras-chave avan√ßadas com m√∫ltiplas t√©cnicas\"\"\"\n",
        "        language = self.detect_language(text)\n",
        "        processed_text = self.advanced_preprocess_text(text, language)\n",
        "\n",
        "        # An√°lise de frequ√™ncia\n",
        "        words = word_tokenize(processed_text)\n",
        "        freq_dist = FreqDist(words)\n",
        "\n",
        "        # Extra√ß√£o com YAKE\n",
        "        yake_extractor = self.yake_extractor_pt if language == 'pt' else self.yake_extractor_en\n",
        "        keywords_yake = [kw for kw, score in yake_extractor.extract_keywords(text)]\n",
        "\n",
        "        # Extra√ß√£o com RAKE\n",
        "        rake = self.rake_pt if language == 'pt' else self.rake_en\n",
        "        rake.extract_keywords_from_text(text)\n",
        "        keywords_rake = rake.get_ranked_phrases()\n",
        "\n",
        "        # Extra√ß√£o com TextRank\n",
        "        nlp = self.nlp_pt if language == 'pt' else self.nlp_en\n",
        "        doc = nlp(text)\n",
        "        keywords_textrank = [phrase.text.lower() for phrase in doc._.phrases]\n",
        "\n",
        "        # N-grams (bigrams e trigrams)\n",
        "        bigrams = [' '.join(gram) for gram in ngrams(words, 2) if len(gram[0]) > 2 and len(gram[1]) > 2]\n",
        "        trigrams = [' '.join(gram) for gram in ngrams(words, 3) if len(gram[0]) > 2 and len(gram[1]) > 2 and len(gram[2]) > 2]\n",
        "\n",
        "        # Combinar todas as keywords\n",
        "        all_keywords = list(set(\n",
        "            keywords_yake[:50] +\n",
        "            keywords_rake[:50] +\n",
        "            keywords_textrank[:50] +\n",
        "            bigrams[:20] +\n",
        "            trigrams[:20] +\n",
        "            [word for word, freq in freq_dist.most_common(30)]\n",
        "        ))\n",
        "\n",
        "        # An√°lise detalhada para debug\n",
        "        self.keyword_analysis = {\n",
        "            'yake': keywords_yake[:20],\n",
        "            'rake': keywords_rake[:20],\n",
        "            'textrank': keywords_textrank[:20],\n",
        "            'bigrams': bigrams[:10],\n",
        "            'trigrams': trigrams[:10],\n",
        "            'frequent': [word for word, freq in freq_dist.most_common(20)]\n",
        "        }\n",
        "\n",
        "        return all_keywords[:max_keywords]\n",
        "\n",
        "    def analyze_keywords(self, keywords):\n",
        "        \"\"\"Analisa e classifica as keywords\"\"\"\n",
        "        # Classificar por tipo (t√©cnica, soft skill, ferramenta, etc.)\n",
        "        technical_terms = []\n",
        "        soft_skills = []\n",
        "        tools = []\n",
        "        languages = []\n",
        "        other = []\n",
        "\n",
        "        # Listas de refer√™ncia (podem ser expandidas)\n",
        "        tech_terms_ref = ['python', 'java', 'javascript', 'sql', 'html', 'css', 'react', 'angular', 'vue', 'node', 'django', 'flask']\n",
        "        soft_skills_ref = ['comunica√ß√£o', 'lideran√ßa', 'trabalho', 'equipe', 'criatividade', 'resolu√ß√£o', 'problemas']\n",
        "        tools_ref = ['git', 'docker', 'jenkins', 'aws', 'azure', 'google cloud', 'linux', 'windows']\n",
        "        languages_ref = ['ingl√™s', 'espanhol', 'franc√™s', 'alem√£o', 'portugu√™s']\n",
        "\n",
        "        for keyword in keywords:\n",
        "            keyword_lower = keyword.lower()\n",
        "\n",
        "            if any(term in keyword_lower for term in tech_terms_ref):\n",
        "                technical_terms.append(keyword)\n",
        "            elif any(term in keyword_lower for term in soft_skills_ref):\n",
        "                soft_skills.append(keyword)\n",
        "            elif any(term in keyword_lower for term in tools_ref):\n",
        "                tools.append(keyword)\n",
        "            elif any(term in keyword_lower for term in languages_ref):\n",
        "                languages.append(keyword)\n",
        "            else:\n",
        "                other.append(keyword)\n",
        "\n",
        "        return {\n",
        "            'technical': technical_terms,\n",
        "            'soft_skills': soft_skills,\n",
        "            'tools': tools,\n",
        "            'languages': languages,\n",
        "            'other': other\n",
        "        }\n",
        "\n",
        "    def process_resume(self):\n",
        "        \"\"\"Processa o curr√≠culo e extrai keywords com an√°lise avan√ßada\"\"\"\n",
        "        print(\"Processando curr√≠culo com an√°lise avan√ßada...\")\n",
        "        self.resume_text = self.extract_text(self.resume_path)\n",
        "\n",
        "        if not self.resume_text:\n",
        "            raise ValueError(\"N√£o foi poss√≠vel extrair texto do curr√≠culo\")\n",
        "\n",
        "        # Extrair keywords\n",
        "        self.resume_keywords = self.extract_advanced_keywords(self.resume_text, max_keywords=200)\n",
        "\n",
        "        # Analisar keywords\n",
        "        keyword_categories = self.analyze_keywords(self.resume_keywords)\n",
        "\n",
        "        # Salvar an√°lise detalhada\n",
        "        with open(KEYWORDS_PATH, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"AN√ÅLISE DETALHADA DE PALAVRAS-CHAVE\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "            f.write(\"CATEGORIAS:\\n\")\n",
        "            for category, words in keyword_categories.items():\n",
        "                f.write(f\"\\n{category.upper()}:\\n\")\n",
        "                f.write(\", \".join(words[:20]) + \"\\n\")\n",
        "\n",
        "            f.write(\"\\n\\nM√âTODOS DE EXTRA√á√ÉO:\\n\")\n",
        "            for method, words in self.keyword_analysis.items():\n",
        "                f.write(f\"\\n{method.upper()}:\\n\")\n",
        "                f.write(\", \".join(words) + \"\\n\")\n",
        "\n",
        "        print(f\"An√°lise completa salva em: {KEYWORDS_PATH}\")\n",
        "        print(f\"Total de palavras-chave extra√≠das: {len(self.resume_keywords)}\")\n",
        "\n",
        "        # Mostrar categorias\n",
        "        print(\"\\nCATEGORIAS DE PALAVRAS-CHAVE:\")\n",
        "        for category, words in keyword_categories.items():\n",
        "            print(f\"   {category.upper()}: {len(words)} palavras\")\n",
        "            if words:\n",
        "                print(f\"      Exemplos: {', '.join(words[:5])}\")\n",
        "\n",
        "        return self.resume_keywords\n",
        "\n",
        "    def search_google_jobs(self, keywords, max_results=20):\n",
        "        \"\"\"Busca vagas no Google\"\"\"\n",
        "        print(\"Buscando vagas no Google...\")\n",
        "        jobs = []\n",
        "\n",
        "        try:\n",
        "            search_query = \"vagas \" + \" OR \".join(keywords[:5])\n",
        "\n",
        "            for url in google_search(search_query, num=max_results, stop=max_results, pause=2):\n",
        "                try:\n",
        "                    # Verificar se √© uma vaga de emprego\n",
        "                    if any(term in url.lower() for term in ['vagas', 'jobs', 'careers', 'emprego', 'carreira']):\n",
        "                        headers = {\n",
        "                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "                        }\n",
        "\n",
        "                        response = requests.get(url, headers=headers, timeout=10)\n",
        "                        if response.status_code == 200:\n",
        "                            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                            # Tentar extrair t√≠tulo\n",
        "                            title = soup.find('title')\n",
        "                            title_text = title.text.strip() if title else 'Vaga encontrada via Google'\n",
        "\n",
        "                            # Descri√ß√£o simplificada\n",
        "                            description = f\"Vaga encontrada: {title_text}\"\n",
        "\n",
        "                            similarity = self.calculate_similarity(description)\n",
        "\n",
        "                            if similarity >= self.min_similarity:\n",
        "                                jobs.append({\n",
        "                                    'title': title_text,\n",
        "                                    'company': 'V√°rias empresas (Google Search)',\n",
        "                                    'location': 'Diversos locais',\n",
        "                                    'description': description[:200] + \"...\",\n",
        "                                    'link': url,\n",
        "                                    'similarity': similarity,\n",
        "                                    'source': 'Google'\n",
        "                                })\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro na busca do Google: {e}\")\n",
        "\n",
        "        return jobs\n",
        "\n",
        "    def search_github_jobs(self, keywords, max_results=20):\n",
        "        \"\"\"Busca vagas na API p√∫blica do GitHub Jobs\"\"\"\n",
        "        print(\"Buscando vagas no GitHub Jobs...\")\n",
        "        jobs = []\n",
        "\n",
        "        try:\n",
        "            search_query = \" \".join(keywords[:3])\n",
        "            url = \"https://jobs.github.com/positions.json\"\n",
        "            params = {\n",
        "                'description': search_query,\n",
        "                'page': 1\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, params=params, timeout=15)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                for job in data[:max_results]:\n",
        "                    job_desc = f\"{job.get('title', '')}. {job.get('description', '')}\"\n",
        "                    similarity = self.calculate_similarity(job_desc)\n",
        "\n",
        "                    if similarity >= self.min_similarity:\n",
        "                        jobs.append({\n",
        "                            'title': job.get('title', 'N/A'),\n",
        "                            'company': job.get('company', 'N/A'),\n",
        "                            'location': job.get('location', 'N/A'),\n",
        "                            'description': job_desc[:300] + \"...\" if len(job_desc) > 300 else job_desc,\n",
        "                            'link': job.get('url', '#'),\n",
        "                            'similarity': similarity,\n",
        "                            'source': 'GitHub Jobs'\n",
        "                        })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro na busca do GitHub Jobs: {e}\")\n",
        "\n",
        "        return jobs\n",
        "\n",
        "    def search_linkedin_simple(self, keywords, max_results=15):\n",
        "        \"\"\"Busca simplificada no LinkedIn\"\"\"\n",
        "        print(\"Buscando vagas no LinkedIn (busca simplificada)...\")\n",
        "        jobs = []\n",
        "\n",
        "        try:\n",
        "            search_query = \" \".join(keywords[:3])\n",
        "            search_url = f\"https://www.linkedin.com/jobs/search/?keywords={quote_plus(search_query)}\"\n",
        "\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "            }\n",
        "\n",
        "            response = requests.get(search_url, headers=headers, timeout=15)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                # Extrair informa√ß√µes b√°sicas (esta √© uma abordagem simplificada)\n",
        "                job_cards = soup.select('.base-card')[:max_results]\n",
        "\n",
        "                for card in job_cards:\n",
        "                    try:\n",
        "                        title_elem = card.select_one('.base-search-card__title')\n",
        "                        title = title_elem.text.strip() if title_elem else 'Vaga no LinkedIn'\n",
        "\n",
        "                        company_elem = card.select_one('.base-search-card__subtitle')\n",
        "                        company = company_elem.text.strip() if company_elem else 'Empresa n√£o especificada'\n",
        "\n",
        "                        location_elem = card.select_one('.job-search-card__location')\n",
        "                        location = location_elem.text.strip() if location_elem else 'Local n√£o especificado'\n",
        "\n",
        "                        link_elem = card.select_one('a.base-card__full-link')\n",
        "                        link = link_elem['href'] if link_elem else '#'\n",
        "\n",
        "                        description = f\"Vaga de {title} na {company}\"\n",
        "                        similarity = self.calculate_similarity(description)\n",
        "\n",
        "                        if similarity >= self.min_similarity:\n",
        "                            jobs.append({\n",
        "                                'title': title,\n",
        "                                'company': company,\n",
        "                                'location': location,\n",
        "                                'description': description,\n",
        "                                'link': link,\n",
        "                                'similarity': similarity,\n",
        "                                'source': 'LinkedIn'\n",
        "                            })\n",
        "\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro na busca do LinkedIn: {e}\")\n",
        "\n",
        "        return jobs\n",
        "\n",
        "    def search_indeed_simple(self, keywords, max_results=15):\n",
        "        \"\"\"Busca simplificada no Indeed\"\"\"\n",
        "        print(\"Buscando vagas no Indeed...\")\n",
        "        jobs = []\n",
        "\n",
        "        try:\n",
        "            search_query = \" \".join(keywords[:3])\n",
        "            search_url = f\"https://br.indeed.com/jobs?q={quote_plus(search_query)}\"\n",
        "\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "            }\n",
        "\n",
        "            response = requests.get(search_url, headers=headers, timeout=15)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                job_cards = soup.select('.jobsearch-SerpJobCard')[:max_results]\n",
        "\n",
        "                for card in job_cards:\n",
        "                    try:\n",
        "                        title_elem = card.select_one('.jobTitle')\n",
        "                        title = title_elem.text.strip() if title_elem else 'Vaga no Indeed'\n",
        "\n",
        "                        company_elem = card.select_one('.companyName')\n",
        "                        company = company_elem.text.strip() if company_elem else 'Empresa n√£o especificada'\n",
        "\n",
        "                        location_elem = card.select_one('.companyLocation')\n",
        "                        location = location_elem.text.strip() if location_elem else 'Local n√£o especificado'\n",
        "\n",
        "                        link_elem = card.select_one('a.jcs-JobTitle')\n",
        "                        link = \"https://br.indeed.com\" + link_elem['href'] if link_elem else '#'\n",
        "\n",
        "                        description = f\"Vaga de {title} na {company}\"\n",
        "                        similarity = self.calculate_similarity(description)\n",
        "\n",
        "                        if similarity >= self.min_similarity:\n",
        "                            jobs.append({\n",
        "                                'title': title,\n",
        "                                'company': company,\n",
        "                                'location': location,\n",
        "                                'description': description,\n",
        "                                'link': link,\n",
        "                                'similarity': similarity,\n",
        "                                'source': 'Indeed'\n",
        "                            })\n",
        "\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro na busca do Indeed: {e}\")\n",
        "\n",
        "        return jobs\n",
        "\n",
        "    def calculate_similarity(self, job_description):\n",
        "        \"\"\"Calcula similaridade entre curr√≠culo e descri√ß√£o da vaga\"\"\"\n",
        "        # Detectar idioma da descri√ß√£o da vaga\n",
        "        job_language = self.detect_language(job_description)\n",
        "\n",
        "        # Pr√©-processar ambos os textos no idioma apropriado\n",
        "        processed_resume = self.advanced_preprocess_text(self.resume_text, self.detect_language(self.resume_text))\n",
        "        processed_job = self.advanced_preprocess_text(job_description, job_language)\n",
        "\n",
        "        # Vetoriza√ß√£o TF-IDF\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform([processed_resume, processed_job])\n",
        "\n",
        "        # Calcular similaridade do cosseno\n",
        "        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "\n",
        "        return round(similarity, 2)\n",
        "\n",
        "    def search_all_jobs(self):\n",
        "        \"\"\"Busca vagas em todas as plataformas\"\"\"\n",
        "        if not self.resume_keywords:\n",
        "            self.process_resume()\n",
        "\n",
        "        all_jobs = []\n",
        "\n",
        "        # Buscar em m√∫ltiplas fontes\n",
        "        print(\"\\nIniciando busca em m√∫ltiplas plataformas...\")\n",
        "\n",
        "        all_jobs.extend(self.search_google_jobs(self.resume_keywords, max_results=25))\n",
        "        all_jobs.extend(self.search_github_jobs(self.resume_keywords, max_results=20))\n",
        "        all_jobs.extend(self.search_linkedin_simple(self.resume_keywords, max_results=20))\n",
        "        all_jobs.extend(self.search_indeed_simple(self.resume_keywords, max_results=20))\n",
        "\n",
        "        # Remover duplicatas baseado no t√≠tulo e empresa\n",
        "        seen = set()\n",
        "        deduplicated_jobs = []\n",
        "        for job in all_jobs:\n",
        "            identifier = (job['title'][:50], job['company'][:30])\n",
        "            if identifier not in seen:\n",
        "                seen.add(identifier)\n",
        "                deduplicated_jobs.append(job)\n",
        "\n",
        "        # Ordenar por similaridade\n",
        "        deduplicated_jobs.sort(key=lambda x: x['similarity'], reverse=True)\n",
        "\n",
        "        self.jobs_found = deduplicated_jobs\n",
        "        return deduplicated_jobs\n",
        "\n",
        "    def export_results(self, output_file=OUTPUT_PATH):\n",
        "        \"\"\"Exporta resultados para Excel\"\"\"\n",
        "        if not self.jobs_found:\n",
        "            print(\"Nenhuma vaga encontrada para exportar\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(self.jobs_found)\n",
        "        df = df[['title', 'company', 'location', 'similarity', 'source', 'link', 'description']]\n",
        "\n",
        "        # Formatar similaridade como porcentagem\n",
        "        df['similarity_percent'] = df['similarity'].apply(lambda x: f\"{x*100:.1f}%\")\n",
        "\n",
        "        df.to_excel(output_file, index=False)\n",
        "        print(f\"Resultados exportados para {output_file}\")\n",
        "\n",
        "    def display_results(self):\n",
        "        \"\"\"Exibe resultados no console\"\"\"\n",
        "        if not self.jobs_found:\n",
        "            print(f\"Nenhuma vaga encontrada com ader√™ncia m√≠nima de {self.min_similarity*100}%\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"VAGAS ENCONTRADAS - {len(self.jobs_found)} resultados\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        for i, job in enumerate(self.jobs_found, 1):\n",
        "            print(f\"\\n{i}. {job['title']}\")\n",
        "            print(f\"   Empresa: {job['company']}\")\n",
        "            print(f\"   Local: {job['location']}\")\n",
        "            print(f\"   Fonte: {job['source']}\")\n",
        "            print(f\"   Ader√™ncia: {job['similarity']*100:.1f}%\")\n",
        "            print(f\"   Link: {job['link']}\")\n",
        "            print(f\"   Descri√ß√£o: {job['description'][:100]}...\")\n",
        "            print(f\"   {'-'*60}\")\n",
        "\n",
        "# Fun√ß√£o principal\n",
        "def main():\n",
        "    # Verificar se o curr√≠culo existe\n",
        "    if not os.path.exists(RESUME_PATH):\n",
        "        print(f\"Arquivo de curr√≠culo n√£o encontrado: {RESUME_PATH}\")\n",
        "        print(\"Por favor, verifique se o arquivo Curriculo.pdf est√° na pasta /content/sample_data/\")\n",
        "        return\n",
        "\n",
        "    # Inicializar o buscador\n",
        "    MIN_SIMILARITY = 0.75  # 75% de ader√™ncia m√≠nima\n",
        "    job_finder = AdvancedJobMatchFinder(RESUME_PATH, MIN_SIMILARITY)\n",
        "\n",
        "    try:\n",
        "        # Processar curr√≠culo\n",
        "        keywords = job_finder.process_resume()\n",
        "\n",
        "        # Buscar vagas\n",
        "        print(\"\\nIniciando busca por vagas em m√∫ltiplas plataformas...\")\n",
        "        jobs = job_finder.search_all_jobs()\n",
        "\n",
        "        # Exibir resultados\n",
        "        job_finder.display_results()\n",
        "\n",
        "        # Exportar resultados\n",
        "        job_finder.export_results()\n",
        "\n",
        "        print(f\"\\nBusca conclu√≠da! {len(jobs)} vagas encontradas.\")\n",
        "        print(f\"Resultados salvos em: {OUTPUT_PATH}\")\n",
        "        print(f\"An√°lise detalhada salva em: {KEYWORDS_PATH}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro durante a execu√ß√£o: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "_yXClxdnvvrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Sistema de Busca de Vagas com Teste Manual de Palavras-Chave\n",
        "Autor: Assistente AI\n",
        "Data: 2024\n",
        "\"\"\"\n",
        "\n",
        "print(\"üîß Configurando ambiente...\")\n",
        "\n",
        "# Instalar bibliotecas essenciais\n",
        "!pip install spacy --quiet\n",
        "!pip install nltk --quiet\n",
        "!pip install scikit-learn --quiet\n",
        "!pip install requests --quiet\n",
        "!pip install beautifulsoup4 --quiet\n",
        "!pip install python-docx --quiet\n",
        "!pip install PyPDF2 --quiet\n",
        "!pip install pandas --quiet\n",
        "!pip install googlesearch-python --quiet\n",
        "\n",
        "# Baixar recursos\n",
        "!python -m spacy download pt_core_news_sm --quiet\n",
        "!python -m spacy download en_core_web_sm --quiet\n",
        "\n",
        "print(\"Ambiente configurado!\")\n",
        "print(\"Importando bibliotecas...\")\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PyPDF2 import PdfReader\n",
        "import docx\n",
        "import spacy\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from urllib.parse import quote_plus\n",
        "from googlesearch import search as google_search\n",
        "import time\n",
        "\n",
        "# Configurar NLTK\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Definir caminhos\n",
        "RESUME_PATH = \"/content/sample_data/Curriculo.pdf\"\n",
        "OUTPUT_PATH = \"/content/sample_data/resultados_vagas.xlsx\"\n",
        "KEYWORDS_PATH = \"/content/sample_data/palavras_chave.txt\"\n",
        "\n",
        "print(\"üîß Criando processador de curr√≠culo...\")\n",
        "\n",
        "class ResumeProcessor:\n",
        "    def __init__(self, resume_path):\n",
        "        self.resume_path = resume_path\n",
        "        self.nlp_pt = spacy.load(\"pt_core_news_sm\")\n",
        "        self.nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "        self.stop_words_pt = set(stopwords.words('portuguese'))\n",
        "        self.stop_words_en = set(stopwords.words('english'))\n",
        "        self.resume_text = \"\"\n",
        "        self.resume_keywords = []\n",
        "\n",
        "    def extract_text_from_pdf(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, 'rb') as file:\n",
        "                pdf_reader = PdfReader(file)\n",
        "                return '\\n'.join([page.extract_text() + \"\\n\" for page in pdf_reader.pages])\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao extrair texto do PDF: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def extract_text_from_docx(self, file_path):\n",
        "        try:\n",
        "            doc = docx.Document(file_path)\n",
        "            return '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao extrair texto do DOCX: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def extract_text(self, file_path):\n",
        "        if file_path.lower().endswith('.pdf'):\n",
        "            return self.extract_text_from_pdf(file_path)\n",
        "        elif file_path.lower().endswith('.docx'):\n",
        "            return self.extract_text_from_docx(file_path)\n",
        "        else:\n",
        "            return \"\"\n",
        "\n",
        "    def detect_language(self, text):\n",
        "        pt_words = ['√©', '√°', '√†', '√£', '√µ', '√ß', 'n√£o', 'para', 'com', 'que']\n",
        "        en_words = ['the', 'and', 'for', 'with', 'that', 'this', 'is', 'are', 'was', 'were']\n",
        "        text_lower = text.lower()\n",
        "        pt_count = sum(1 for word in pt_words if word in text_lower)\n",
        "        en_count = sum(1 for word in en_words if word in text_lower)\n",
        "        return 'pt' if pt_count > en_count else 'en'\n",
        "\n",
        "    def preprocess_text(self, text, language='pt'):\n",
        "        text = text.lower()\n",
        "        if language == 'pt':\n",
        "            text = re.sub(r'[^a-z√°√†√¢√£√©√®√™√≠√Ø√≥√¥√µ√∂√∫√ß√±\\s]', ' ', text)\n",
        "        else:\n",
        "            text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "        text = re.sub(r'https?://\\S+|www\\.\\S+|\\S+@\\S+', ' ', text)\n",
        "        tokens = word_tokenize(text)\n",
        "        stop_words = self.stop_words_pt if language == 'pt' else self.stop_words_en\n",
        "        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def extract_keywords(self, text, max_keywords=100):\n",
        "        language = self.detect_language(text)\n",
        "        processed_text = self.preprocess_text(text, language)\n",
        "        words = word_tokenize(processed_text)\n",
        "        freq_dist = FreqDist(words)\n",
        "        bigrams = [' '.join(gram) for gram in ngrams(words, 2)]\n",
        "        keywords = [word for word, freq in freq_dist.most_common(50)] + bigrams[:20]\n",
        "        return list(set(keywords))[:max_keywords]\n",
        "\n",
        "    def process_resume(self):\n",
        "        print(\"Processando curr√≠culo...\")\n",
        "        self.resume_text = self.extract_text(self.resume_path)\n",
        "        if not self.resume_text:\n",
        "            raise ValueError(\"N√£o foi poss√≠vel extrair texto do curr√≠culo\")\n",
        "        self.resume_keywords = self.extract_keywords(self.resume_text)\n",
        "        with open(KEYWORDS_PATH, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"\\n\".join(self.resume_keywords))\n",
        "        print(f\"{len(self.resume_keywords)} palavras-chave extra√≠das\")\n",
        "        print(f\"Salvas em: {KEYWORDS_PATH}\")\n",
        "        print(f\"Exemplos: {', '.join(self.resume_keywords[:10])}\")\n",
        "        return self.resume_keywords\n",
        "\n",
        "print(\"üîß Criando buscador de vagas...\")\n",
        "\n",
        "class JobSearcher:\n",
        "    def __init__(self, keywords, min_similarity=0.6):\n",
        "        self.keywords = [kw.lower() for kw in keywords]\n",
        "        self.min_similarity = min_similarity\n",
        "        self.jobs_found = []\n",
        "\n",
        "    def calculate_similarity(self, job_description):\n",
        "        job_words = set(re.findall(r'\\w+', job_description.lower()))\n",
        "        if not job_words:\n",
        "            return 0\n",
        "        common_words = job_words.intersection(set(self.keywords))\n",
        "        return round(len(common_words) / len(job_words), 2)\n",
        "\n",
        "    def search_github_jobs(self, max_results=15):\n",
        "        print(\"Buscando no GitHub Jobs...\")\n",
        "        jobs = []\n",
        "        try:\n",
        "            search_query = \" \".join(self.keywords[:3])\n",
        "            response = requests.get(\"https://jobs.github.com/positions.json\",\n",
        "                                  params={'description': search_query}, timeout=15)\n",
        "            if response.status_code == 200:\n",
        "                for job in response.json()[:max_results]:\n",
        "                    job_desc = f\"{job.get('title', '')}. {job.get('description', '')}\"\n",
        "                    similarity = self.calculate_similarity(job_desc)\n",
        "                    if similarity >= self.min_similarity:\n",
        "                        jobs.append({\n",
        "                            'title': job.get('title', 'N/A'),\n",
        "                            'company': job.get('company', 'N/A'),\n",
        "                            'location': job.get('location', 'N/A'),\n",
        "                            'description': job_desc[:200] + \"...\",\n",
        "                            'link': job.get('url', '#'),\n",
        "                            'similarity': similarity,\n",
        "                            'source': 'GitHub Jobs'\n",
        "                        })\n",
        "        except Exception as e:\n",
        "            print(f\"Erro no GitHub Jobs: {e}\")\n",
        "        return jobs\n",
        "\n",
        "    def search_indeed_simple(self, max_results=10):\n",
        "        print(\"Buscando no Indeed...\")\n",
        "        jobs = []\n",
        "        try:\n",
        "            search_query = \" \".join(self.keywords[:2])\n",
        "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
        "            response = requests.get(f\"https://br.indeed.com/jobs?q={quote_plus(search_query)}\",\n",
        "                                  headers=headers, timeout=15)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                for card in soup.select('.jobsearch-SerpJobCard')[:max_results]:\n",
        "                    try:\n",
        "                        title = card.select_one('.jobTitle').text.strip() if card.select_one('.jobTitle') else 'Vaga no Indeed'\n",
        "                        company = card.select_one('.companyName').text.strip() if card.select_one('.companyName') else 'Empresa'\n",
        "                        location = card.select_one('.companyLocation').text.strip() if card.select_one('.companyLocation') else 'Local'\n",
        "                        link = \"https://br.indeed.com\" + card.select_one('a.jcs-JobTitle')['href'] if card.select_one('a.jcs-JobTitle') else '#'\n",
        "                        description = f\"Vaga de {title} na {company}\"\n",
        "                        similarity = self.calculate_similarity(description)\n",
        "                        if similarity >= self.min_similarity:\n",
        "                            jobs.append({\n",
        "                                'title': title, 'company': company, 'location': location,\n",
        "                                'description': description, 'link': link,\n",
        "                                'similarity': similarity, 'source': 'Indeed'\n",
        "                            })\n",
        "                    except:\n",
        "                        continue\n",
        "        except Exception as e:\n",
        "            print(f\"Erro no Indeed: {e}\")\n",
        "        return jobs\n",
        "\n",
        "    def search_google_simple(self, max_results=10):\n",
        "        print(\"Buscando no Google...\")\n",
        "        jobs = []\n",
        "        try:\n",
        "            search_query = \"vagas \" + \" \".join(self.keywords[:3])\n",
        "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
        "            response = requests.get(f\"https://www.google.com/search?q={quote_plus(search_query)}\",\n",
        "                                  headers=headers, timeout=15)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                for result in soup.select('.g')[:max_results]:\n",
        "                    try:\n",
        "                        title = result.select_one('h3').text.strip() if result.select_one('h3') else 'Vaga encontrada'\n",
        "                        link = result.select_one('a')['href'] if result.select_one('a') else '#'\n",
        "                        description = f\"Vaga: {title}\"\n",
        "                        similarity = self.calculate_similarity(description)\n",
        "                        if similarity >= self.min_similarity:\n",
        "                            jobs.append({\n",
        "                                'title': title, 'company': 'V√°rias empresas',\n",
        "                                'location': 'Diversos locais', 'description': description,\n",
        "                                'link': link, 'similarity': similarity, 'source': 'Google'\n",
        "                            })\n",
        "                    except:\n",
        "                        continue\n",
        "        except Exception as e:\n",
        "            print(f\"Erro no Google: {e}\")\n",
        "        return jobs\n",
        "\n",
        "    def search_all_jobs(self):\n",
        "        all_jobs = []\n",
        "        all_jobs.extend(self.search_github_jobs())\n",
        "        all_jobs.extend(self.search_indeed_simple())\n",
        "        all_jobs.extend(self.search_google_simple())\n",
        "        seen = set()\n",
        "        unique_jobs = []\n",
        "        for job in all_jobs:\n",
        "            identifier = (job['title'][:30], job['company'][:20])\n",
        "            if identifier not in seen:\n",
        "                seen.add(identifier)\n",
        "                unique_jobs.append(job)\n",
        "        unique_jobs.sort(key=lambda x: x['similarity'], reverse=True)\n",
        "        self.jobs_found = unique_jobs\n",
        "        return unique_jobs\n",
        "\n",
        "    def export_results(self, output_file=OUTPUT_PATH):\n",
        "        if not self.jobs_found:\n",
        "            print(\"Nenhuma vaga encontrada\")\n",
        "            return\n",
        "        df = pd.DataFrame(self.jobs_found)\n",
        "        df['similarity_percent'] = df['similarity'].apply(lambda x: f\"{x*100:.1f}%\")\n",
        "        df.to_excel(output_file, index=False)\n",
        "        print(f\"Resultados exportados: {output_file}\")\n",
        "\n",
        "    def display_results(self):\n",
        "        if not self.jobs_found:\n",
        "            print(\"Nenhuma vaga encontrada\")\n",
        "            return\n",
        "        print(f\"\\n{len(self.jobs_found)} VAGAS ENCONTRADAS\")\n",
        "        print(\"=\" * 60)\n",
        "        for i, job in enumerate(self.jobs_found, 1):\n",
        "            print(f\"\\n{i}. {job['title']}\")\n",
        "            print(f\"   {job['company']}\")\n",
        "            print(f\"   {job['location']}\")\n",
        "            print(f\"   {job['similarity']*100:.1f}% de ader√™ncia\")\n",
        "            print(f\"   {job['link']}\")\n",
        "            print(f\"   {job['description'][:80]}...\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "# TESTE MANUAL DE PALAVRAS-CHAVE\n",
        "def test_keywords_manually():\n",
        "    print(\"\\nTESTE MANUAL DE PALAVRAS-CHAVE\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Carregar palavras-chave existentes se dispon√≠vel\n",
        "    try:\n",
        "        with open(KEYWORDS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "            existing_keywords = [line.strip() for line in f if line.strip()]\n",
        "        print(f\"{len(existing_keywords)} palavras-chave existentes carregadas\")\n",
        "    except:\n",
        "        existing_keywords = []\n",
        "        print(\"Nenhum arquivo de palavras-chave encontrado\")\n",
        "\n",
        "    print(\"\\nOp√ß√µes:\")\n",
        "    print(\"1. Usar palavras-chave existentes do arquivo\")\n",
        "    print(\"2. Digitar novas palavras-chave manualmente\")\n",
        "    print(\"3. Usar palavras-chave de exemplo\")\n",
        "\n",
        "    choice = input(\"\\nEscolha (1/2/3): \").strip()\n",
        "\n",
        "    if choice == \"1\" and existing_keywords:\n",
        "        keywords = existing_keywords\n",
        "        print(f\"Usando {len(keywords)} palavras-chave do arquivo\")\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        print(\"\\nDigite as palavras-chave separadas por V√çRGULA\")\n",
        "        print(\"Exemplo: python, java, desenvolvedor, analista de dados, sql\")\n",
        "        user_input = input(\"Palavras-chave: \")\n",
        "        keywords = [kw.strip() for kw in user_input.split(\",\") if kw.strip()]\n",
        "        print(f\"{len(keywords)} palavras-chave inseridas\")\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        # Palavras-chave de exemplo\n",
        "        keywords = [\"python\", \"java\", \"desenvolvedor\", \"analista de dados\", \"sql\",\n",
        "                   \"machine learning\", \"web development\", \"backend\", \"frontend\", \"fullstack\"]\n",
        "        print(f\"Usando {len(keywords)} palavras-chave de exemplo\")\n",
        "\n",
        "    else:\n",
        "        print(\"Op√ß√£o inv√°lida, usando palavras-chave de exemplo\")\n",
        "        keywords = [\"python\", \"java\", \"desenvolvedor\", \"analista\", \"programador\"]\n",
        "\n",
        "    print(f\"\\nPalavras-chave para busca:\")\n",
        "    print(\", \".join(keywords))\n",
        "\n",
        "    # Configurar busca\n",
        "    try:\n",
        "        min_similarity = float(input(\"\\nSimilaridade m√≠nima (0.0-1.0) [0.5]: \") or \"0.5\")\n",
        "    except:\n",
        "        min_similarity = 0.5\n",
        "        print(\"Usando similaridade padr√£o: 0.5\")\n",
        "\n",
        "    # Executar busca\n",
        "    print(\"\\nIniciando busca de vagas...\")\n",
        "    searcher = JobSearcher(keywords, min_similarity=min_similarity)\n",
        "    jobs = searcher.search_all_jobs()\n",
        "    searcher.display_results()\n",
        "    searcher.export_results()\n",
        "\n",
        "    print(f\"\\nBusca conclu√≠da! {len(jobs)} vagas encontradas.\")\n",
        "\n",
        "# PROCESSO PRINCIPAL\n",
        "print(\"SISTEMA DE BUSCA DE VAGAS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Verificar se o curr√≠culo existe\n",
        "if not os.path.exists(RESUME_PATH):\n",
        "    print(f\"Arquivo n√£o encontrado: {RESUME_PATH}\")\n",
        "    print(\"Coloque seu curr√≠culo em PDF na pasta /content/sample_data/\")\n",
        "    print(\"\\nIniciando teste manual de palavras-chave...\")\n",
        "    test_keywords_manually()\n",
        "else:\n",
        "    print(\"Op√ß√µes dispon√≠veis:\")\n",
        "    print(\"1. Processar curr√≠culo automaticamente\")\n",
        "    print(\"2. Teste manual de palavras-chave\")\n",
        "    print(\"3. Ambos (processar e depois testar)\")\n",
        "\n",
        "    choice = input(\"\\nEscolha (1/2/3): \").strip()\n",
        "\n",
        "    if choice == \"1\":\n",
        "        # Processar curr√≠culo automaticamente\n",
        "        try:\n",
        "            processor = ResumeProcessor(RESUME_PATH)\n",
        "            keywords = processor.process_resume()\n",
        "            print(\"\\nBuscando vagas...\")\n",
        "            searcher = JobSearcher(keywords, min_similarity=0.5)\n",
        "            jobs = searcher.search_all_jobs()\n",
        "            searcher.display_results()\n",
        "            searcher.export_results()\n",
        "            print(f\"\\nProcesso conclu√≠do! {len(jobs)} vagas encontradas.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Erro no processamento: {e}\")\n",
        "            print(\"Iniciando teste manual...\")\n",
        "            test_keywords_manually()\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        # Apenas teste manual\n",
        "        test_keywords_manually()\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        # Ambos: processar e depois testar\n",
        "        try:\n",
        "            processor = ResumeProcessor(RESUME_PATH)\n",
        "            keywords = processor.process_resume()\n",
        "            print(\"\\nBuscando vagas com palavras extra√≠das...\")\n",
        "            searcher = JobSearcher(keywords, min_similarity=0.5)\n",
        "            jobs = searcher.search_all_jobs()\n",
        "            searcher.display_results()\n",
        "            searcher.export_results()\n",
        "            print(f\"\\nProcesso autom√°tico conclu√≠do! {len(jobs)} vagas encontradas.\")\n",
        "\n",
        "            # Agora teste manual\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"AGORA TESTE MANUAL DE PALAVRAS-CHAVE\")\n",
        "            print(\"=\"*50)\n",
        "            test_keywords_manually()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro no processamento: {e}\")\n",
        "            print(\"Iniciando teste manual...\")\n",
        "            test_keywords_manually()\n",
        "    else:\n",
        "        print(\"Op√ß√£o inv√°lida, iniciando teste manual...\")\n",
        "        test_keywords_manually()\n",
        "\n",
        "print(\"\\nDICAS:\")\n",
        "print(\"- Para melhores resultados, use palavras-chave espec√≠ficas\")\n",
        "print(\"- Experimente com diferentes n√≠veis de similaridade\")\n",
        "print(\"- Combine palavras t√©cnicas com habilidades espec√≠ficas\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVmwgA64D3nh",
        "outputId": "f86ebb2c-d6ad-4de7-cde5-910805badc06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Configurando ambiente...\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Ambiente configurado!\n",
            "Importando bibliotecas...\n",
            "Criando processador de curr√≠culo...\n",
            "Criando buscador de vagas...\n",
            "SISTEMA DE BUSCA DE VAGAS\n",
            "==================================================\n",
            "Arquivo n√£o encontrado: /content/sample_data/Curriculo.pdf\n",
            "Coloque seu curr√≠culo em PDF na pasta /content/sample_data/\n",
            "\n",
            "Iniciando teste manual de palavras-chave...\n",
            "\n",
            "TESTE MANUAL DE PALAVRAS-CHAVE\n",
            "==================================================\n",
            "Nenhum arquivo de palavras-chave encontrado\n",
            "\n",
            "Op√ß√µes:\n",
            "1. Usar palavras-chave existentes do arquivo\n",
            "2. Digitar novas palavras-chave manualmente\n",
            "3. Usar palavras-chave de exemplo\n",
            "\n",
            "Escolha (1/2/3): 2\n",
            "\n",
            "Digite as palavras-chave separadas por V√çRGULA\n",
            "Exemplo: python, java, desenvolvedor, analista de dados, sql\n",
            "Palavras-chave: SQL, Python, R, Power BI, Tableau, Qlik Sense, Excel, Power Query, Power Pivot, DAX, MDX, ETL, ELT, Data Warehousing, Data Lakes, Data Modeling, Star Schema, Snowflake Schema, OLAP, Big Data, Spark, PySpark, Hadoop, Kafka, Airflow, Azure Data Factory, AWS Glue, Google BigQuery, Machine Learning, Deep Learning, Neural Networks, NLP, Computer Vision, Scikit-learn, TensorFlow, PyTorch, Regress√£o Linear, Regress√£o Log√≠stica, Classifica√ß√£o, Clusteriza√ß√£o, S√©ries Temporais, An√°lise Preditiva, Estat√≠stica, Google Analytics, Adobe Analytics, Data Visualization, Storytelling com Dados, A/B Testing, Cohort Analysis, Segmenta√ß√£o de Clientes, KPI Design, Data Quality, Data Cleaning, Data Governance, Data Architecture, DataOps, MLOps, CI/CD, Docker, Kubernetes, Git, Jupyter Notebook, Databricks, Snowflake, Oracle Database, PostgreSQL, MySQL, SQL Server, MongoDB, Cassandra, NoSQL, APIs, REST APIs, Web Scraping, Selenium, Beautiful Soup, Apache Beam, Presto, Trino, Hive, HBase, Scala, Linux, Bash Scripting, SAP, ERP, MicroStrategy, Google Looker, SAS, IBM Cognos, Apache Superset, Metabase, Alteryx, Knime, Talend, Informatica, SSIS, SSRS, SSAS, Excel Avan√ßado, VBA, Macros, Power Platform, Microsoft Fabric, Datadog, Splunk, Elasticsearch, Prometheus, Grafana, Tableau Prep, Alteryx Designer, Matplotlib, Seaborn, Plotly, ggplot2, Pandas, NumPy, SciPy, SciKit-Learn, NLTK, SpaCy, OpenCV, TensorFlow Serving, MLflow, Kubeflow, Amazon SageMaker, Azure ML, Google AI Platform, H2O.ai, DataRobot, Snowpark, dbt, Great Expectations, Monte Carlo, Prefect, Dagster, Fivetran, Stitch Data, Segment, Mixpanel, Amplitude, Pendo, Looker Studio, Google Data Studio, IBM Planning Analytics, Anaplan, Hyperion, SAP BW, SAP HANA, SAP Analytics Cloud, Oracle Hyperion, TM1, Board, Prophix, Jedox, Infor Birst, Sisense, GoodData, Domo, Yellowfin, ThoughtSpot, Mode Analytics, Redash, Periscope Data, Chartio, Sigma Computing, LookML, Power Apps, Power Automate, SharePoint, Microsoft Teams, Slack, Zapier, Integromat, UiPath, Automation Anywhere, Blue Prism, Python RPA, SQL Server Management Studio, pgAdmin, MySQL Workbench, DBeaver, DataGrip, ER/Studio, ERwin, Toad, SQL Developer, Navicat, HeidiSQL, Beekeeper Studio, DbVisualizer, Aqua Data Studio, MongoDB Compass, Robo 3T, Studio 3T, PostgreSQL CLI, MySQL CLI, SQLite, Apache Cassandra, Redis, Couchbase, DynamoDB, Firebase, Cosmos DB, Bigtable, Amazon Redshift, Azure Synapse Analytics, Snowflake, Google BigQuery, Teradata, Vertica, Exasol, Greenplum, ClickHouse, DuckDB, MariaDB, Percona, CockroachDB, YugabyteDB, TimescaleDB, InfluxDB, Prometheus, Graphite, Grafana, Kibana, Tableau Server, Power BI Service, Qlik Sense Enterprise, Looker, Metabase, Apache Superset, Redash, Mode Analytics, Hex, Deepnote, Noteable, Count, Polymer, Amplitude, Mixpanel, Heap, Pendo, FullStory, Hotjar, Google Optimize, Optimizely, VWO, AB Tasty, Kameleoon, Dynamic Yield, Evergage, Segment, mParticle, RudderStack, Tealium, Ensighten, Segment Personas, Salesforce DMP, Adobe Audience Manager, Oracle BlueKai, Lotame, Nielsen DMP, Comscore, MediaMath, The Trade Desk, Google Marketing Platform, Facebook Analytics, TikTok Analytics, Snapchat Analytics, Pinterest Analytics, LinkedIn Analytics, Twitter Analytics, Shopify Analytics, WooCommerce Analytics, Magento Analytics, BigCommerce Analytics, Salesforce Analytics, HubSpot Analytics, Marketo Analytics, Pardot Analytics, Eloqua Analytics, Zoho Analytics, Freshworks Analytics, ServiceNow Analytics, Jira Analytics, Asana Analytics, Trello Analytics, Monday.com Analytics, Smartsheet Analytics, Airtable Analytics, Notion Analytics, Coda Analytics, Quip Analytics, Confluence Analytics, Slack Analytics, Microsoft Teams Analytics, Zoom Analytics, Webex Analytics, Google Meet Analytics, Calendly Analytics, Acuity Scheduling Analytics, HubSpot CRM Analytics, Salesforce CRM Analytics, Zoho CRM Analytics, Freshsales Analytics, Pipedrive Analytics, Insightly Analytics, Nimble Analytics, Agile CRM Analytics, Keap Analytics, ActiveCampaign Analytics, Mailchimp Analytics, Constant Contact Analytics, Sendinblue Analytics, GetResponse Analytics, ConvertKit Analytics, AWeber Analytics, Campaign Monitor Analytics, Drip Analytics, Klaviyo Analytics, Omnisend Analytics, Attentive Analytics, Postscript Analytics, SMSBump Analytics, Yotpo Analytics, Judge.me Analytics, Loox Analytics, Stamped Analytics, Reviews.io Analytics, Trustpilot Analytics, G2 Analytics, Capterra Analytics, Gartner Analytics, Forrester Analytics, IDC Analytics, Nielsen Analytics, Comscore Analytics, Kantar Analytics, Ipsos Analytics, GfK Analytics, YouGov Analytics, Statista Analytics, SimilarWeb Analytics, SEMrush Analytics, Ahrefs Analytics, Moz Analytics, Majestic SEO Analytics, SpyFu Analytics, Keyword Planner, Google Search Console, Bing Webmaster Tools, Yandex Webmaster, Screaming Frog, DeepCrawl, Sitebulb, Botify, OnCrawl, Rank Ranger, SEO PowerSuite, Serpstat, Mangools, LongTail Pro, Keyword Tool, AnswerThePublic, Ubersuggest, BuzzSumo, ContentKing, ContentSquare, Glassbox, Decibel, FullStory, Hotjar, Lucky Orange, Mouseflow, SessionCam, Smartlook, Inspectlet, Crazy Egg, VWO Insights, AB Tasty, Optimizely, Google Analytics, Adobe Analytics, Mixpanel, Amplitude, Heap, Pendo, Countly, Matomo, Open Web Analytics, Piwik PRO, Fathom Analytics, Simple Analytics, Plausible Analytics, Umami, GoatCounter, Cloudflare Analytics, Fastly Insights, Akamai mPulse, Catchpoint, Dynatrace, New Relic, AppDynamics, Datadog, Splunk, Elasticsearch, Logstash, Kibana, Graylog, Sumo Logic, Loggly, Papertrail, Sentry, Rollbar, Bugsnag, Airbrake, Raygun, Instana, Honeycomb, Lightstep, SignalFx, Coralogix, Logz.io, Humio, Chronicle, Google Cloud Logging, AWS CloudTrail, Azure Monitor, IBM Cloud Monitoring, Oracle Cloud Monitoring, Alibaba Cloud Monitoring, DigitalOcean Monitoring, Heroku Metrics, Netlify Analytics, Vercel Analytics, Cloudflare Analytics, AWS Cost Explorer, Azure Cost Management, Google Cost Management, CloudHealth, Cloudability, Apptio, Kubecost, VMware vRealize, Turbonomic, Spot by NetApp, ParkMyCloud, CloudCheckr, Nutanix Beam, Azure Arc, AWS Systems Manager, Google Cloud Operations, Datadog, New Relic, AppDynamics, Dynatrace, Instana, Honeycomb, Lightstep, SignalFx, Coralogix, Logz.io, Splunk, Elasticsearch, Sumo Logic, Graylog, Loggly, Papertrail, Sentry, Rollbar, Bugsnag, Airbrake, Raygun, Instabug, Crashlytics, Firebase Crashlytics, Microsoft App Center, TestFlight, Google Play Console, Apple App Store Connect, ASO, Sensor Tower, App Annie, Appfigures, MobileAction, Priori Data, Apptopia, Adjust, AppsFlyer, Branch, Kochava, Singular, Tenjin, ironSource, Unity Analytics, GameAnalytics, DeltaDNA, PlayFab, Google Play Games, Apple Game Center, Steamworks, Epic Games Store, Xbox Live, PlayStation Network, Nintendo Switch, Twitch, Discord, YouTube Gaming, Facebook Gaming, Mixer, Caffeine, DLive, Trovo, Nimo TV, Bigo Live, HUYA, Douyu, Twitch Analytics, YouTube Analytics, Facebook Gaming Analytics, Mixer Analytics, Caffeine Analytics, DLive Analytics, Trovo Analytics, Nimo TV Analytics, Bigo Live Analytics, HUYA Analytics, Douyu Analytics, Steam Analytics, Epic Games Store Analytics, Xbox Analytics, PlayStation Analytics, Nintendo Analytics, Google Stadia Analytics, NVIDIA GeForce Now Analytics, Amazon Luna Analytics, Xbox Cloud Gaming Analytics, PlayStation Now Analytics, Shadow Analytics, Vortex Analytics, Parsec Analytics, Rainway Analytics, Moonlight Analytics, Steam Link Analytics, Remote Play Analytics, Chromecast Analytics, AirPlay Analytics, Miracast Analytics, DLNA Analytics, Plex Analytics, Emby Analytics, Jellyfin Analytics, Kodi Analytics, VLC Analytics, QuickTime Analytics, Windows Media Player Analytics, iTunes Analytics, Google Play Music Analytics, Spotify Analytics, Apple Music Analytics, Amazon Music Analytics, YouTube Music Analytics, Deezer Analytics, Tidal Analytics, Pandora Analytics, SoundCloud Analytics, Bandcamp Analytics, Audius Analytics, Resonate Analytics, Amuse Analytics, DistroKid Analytics, TuneCore Analytics, CD Baby Analytics, Ditto Music Analytics, Record Union Analytics, Symphonic Distribution Analytics, Believe Digital Analytics, ADA Worldwide Analytics, The Orchard Analytics, Ingrooves Analytics, Empire Distribution Analytics, Create Music Group Analytics, AWAL Analytics, Kobalt Analytics, Sony Music Analytics, Universal Music Group Analytics, Warner Music Group Analytics, BMG Analytics, Concord Analytics, [email protected] Analytics, Alpha Data Analytics, SoundScan Analytics, BuzzAngle Analytics, Midia Research Analytics, IFPI Analytics, RIAA Analytics, BPI Analytics, ARIA Analytics, BVMI Analytics, SNEP Analytics, PROMUSICAE Analytics, FIMI Analytics, AFP Analytics, GfK Entertainment Analytics, Official Charts Company Analytics, Billboard Analytics, Rolling Stone Analytics, Pitchfork Analytics, Metacritic Analytics, Album of the Year Analytics, RateYourMusic Analytics, Discogs Analytics, MusicBrainz Analytics, AllMusic Analytics, Genius Analytics, Shazam Analytics, SoundHound Analytics, Musixmatch Analytics, Spotify for Artists Analytics, Apple Music for Artists Analytics, YouTube Analytics for Artists, Amazon Music for Artists Analytics, Deezer for Creators Analytics, Tidal for Artists Analytics, SoundCloud for Artists Analytics, Bandcamp for Artists Analytics, Audius for Artists Analytics, Amuse for Artists Analytics, DistroKid for Artists Analytics, TuneCore for Artists Analytics, CD Baby for Artists Analytics, Ditto Music for Artists Analytics, Record Union for Artists Analytics, Symphonic Distribution for Artists Analytics, Believe Digital for Artists Analytics, ADA Worldwide for Artists Analytics, The Orchard for Artists Analytics, Ingrooves for Artists Analytics, Empire Distribution for Artists Analytics, Create Music Group for Artists Analytics, AWAL for Artists Analytics, Kobalt for Artists Analytics, Sony Music for Artists Analytics, Universal Music for Artists Analytics, Warner Music for Artists Analytics, BMG for Artists Analytics, Concord for Artists Analytics, [email protected] for Artists Analytics, Alpha Data for Artists Analytics, SoundScan for Artists Analytics, BuzzAngle for Artists Analytics, Midia Research for Artists Analytics, IFPI for Artists Analytics, RIAA for Artists Analytics, BPI for Artists Analytics, ARIA for Artists Analytics, BVMI for Artists Analytics, SNEP for Artists Analytics, PROMUSICAE for Artists Analytics, FIMI for Artists Analytics, AFP for Artists Analytics, GfK Entertainment for Artists Analytics, Official Charts Company for Artists Analytics, Billboard for Artists Analytics, Rolling Stone for Artists Analytics, Pitchfork for Artists Analytics, Metacritic for Artists Analytics, Album of the Year for Artists Analytics, RateYourMusic for Artists Analytics, Discogs for Artists Analytics, MusicBrainz for Artists Analytics, AllMusic for Artists Analytics, Genius for Artists Analytics, Shazam for Artists Analytics, SoundHound for Artists Analytics, Musixmatch for Artists Analytics, Spotify for Podcasters Analytics, Apple Podcasts Analytics, Google Podcasts Analytics, Anchor Analytics, Spreaker Analytics, Libsyn Analytics, Blubrry Analytics, Podbean Analytics, Buzzsprout Analytics, Transistor Analytics, Simplecast Analytics, Castos Analytics, Captivate Analytics, Podomatic Analytics, Audioboom Analytics, Megaphone Analytics, Omny Studio Analytics, Acast Analytics, Podcorn Analytics, Gumball Analytics, Giphy Analytics, Tenor Analytics, Imgur Analytics, Flickr Analytics, 500px Analytics, Unsplash Analytics, Pexels Analytics, Pixabay Analytics, Shutterstock Analytics, Getty Images Analytics, Adobe Stock Analytics, iStock Analytics, Dreamstime Analytics, Depositphotos Analytics, Canva Analytics, Figma Analytics, Sketch Analytics, Adobe XD Analytics, InVision Analytics, Marvel Analytics, Framer Analytics, Webflow Analytics, Bubble Analytics, Wix Analytics, Squarespace Analytics, Weebly Analytics, WordPress Analytics, Joomla Analytics, Drupal Analytics, Magento Analytics, Shopify Analytics, BigCommerce Analytics, WooCommerce Analytics, PrestaShop Analytics, OpenCart Analytics, Ecwid Analytics, Square Online Analytics, GoDaddy Analytics, Namecheap Analytics, Google Domains Analytics, Cloudflare Registrar Analytics, Hover Analytics, Gandi Analytics, OVHcloud Analytics, Hetzner Analytics, DigitalOcean Analytics, Linode Analytics, Vultr Analytics, AWS Lightsail Analytics, Google Cloud Platform Analytics, Microsoft Azure Analytics, IBM Cloud Analytics, Oracle Cloud Analytics, Alibaba Cloud Analytics,\n",
            "750 palavras-chave inseridas\n",
            "\n",
            "Palavras-chave para busca:\n",
            "SQL, Python, R, Power BI, Tableau, Qlik Sense, Excel, Power Query, Power Pivot, DAX, MDX, ETL, ELT, Data Warehousing, Data Lakes, Data Modeling, Star Schema, Snowflake Schema, OLAP, Big Data, Spark, PySpark, Hadoop, Kafka, Airflow, Azure Data Factory, AWS Glue, Google BigQuery, Machine Learning, Deep Learning, Neural Networks, NLP, Computer Vision, Scikit-learn, TensorFlow, PyTorch, Regress√£o Linear, Regress√£o Log√≠stica, Classifica√ß√£o, Clusteriza√ß√£o, S√©ries Temporais, An√°lise Preditiva, Estat√≠stica, Google Analytics, Adobe Analytics, Data Visualization, Storytelling com Dados, A/B Testing, Cohort Analysis, Segmenta√ß√£o de Clientes, KPI Design, Data Quality, Data Cleaning, Data Governance, Data Architecture, DataOps, MLOps, CI/CD, Docker, Kubernetes, Git, Jupyter Notebook, Databricks, Snowflake, Oracle Database, PostgreSQL, MySQL, SQL Server, MongoDB, Cassandra, NoSQL, APIs, REST APIs, Web Scraping, Selenium, Beautiful Soup, Apache Beam, Presto, Trino, Hive, HBase, Scala, Linux, Bash Scripting, SAP, ERP, MicroStrategy, Google Looker, SAS, IBM Cognos, Apache Superset, Metabase, Alteryx, Knime, Talend, Informatica, SSIS, SSRS, SSAS, Excel Avan√ßado, VBA, Macros, Power Platform, Microsoft Fabric, Datadog, Splunk, Elasticsearch, Prometheus, Grafana, Tableau Prep, Alteryx Designer, Matplotlib, Seaborn, Plotly, ggplot2, Pandas, NumPy, SciPy, SciKit-Learn, NLTK, SpaCy, OpenCV, TensorFlow Serving, MLflow, Kubeflow, Amazon SageMaker, Azure ML, Google AI Platform, H2O.ai, DataRobot, Snowpark, dbt, Great Expectations, Monte Carlo, Prefect, Dagster, Fivetran, Stitch Data, Segment, Mixpanel, Amplitude, Pendo, Looker Studio, Google Data Studio, IBM Planning Analytics, Anaplan, Hyperion, SAP BW, SAP HANA, SAP Analytics Cloud, Oracle Hyperion, TM1, Board, Prophix, Jedox, Infor Birst, Sisense, GoodData, Domo, Yellowfin, ThoughtSpot, Mode Analytics, Redash, Periscope Data, Chartio, Sigma Computing, LookML, Power Apps, Power Automate, SharePoint, Microsoft Teams, Slack, Zapier, Integromat, UiPath, Automation Anywhere, Blue Prism, Python RPA, SQL Server Management Studio, pgAdmin, MySQL Workbench, DBeaver, DataGrip, ER/Studio, ERwin, Toad, SQL Developer, Navicat, HeidiSQL, Beekeeper Studio, DbVisualizer, Aqua Data Studio, MongoDB Compass, Robo 3T, Studio 3T, PostgreSQL CLI, MySQL CLI, SQLite, Apache Cassandra, Redis, Couchbase, DynamoDB, Firebase, Cosmos DB, Bigtable, Amazon Redshift, Azure Synapse Analytics, Snowflake, Google BigQuery, Teradata, Vertica, Exasol, Greenplum, ClickHouse, DuckDB, MariaDB, Percona, CockroachDB, YugabyteDB, TimescaleDB, InfluxDB, Prometheus, Graphite, Grafana, Kibana, Tableau Server, Power BI Service, Qlik Sense Enterprise, Looker, Metabase, Apache Superset, Redash, Mode Analytics, Hex, Deepnote, Noteable, Count, Polymer, Amplitude, Mixpanel, Heap, Pendo, FullStory, Hotjar, Google Optimize, Optimizely, VWO, AB Tasty, Kameleoon, Dynamic Yield, Evergage, Segment, mParticle, RudderStack, Tealium, Ensighten, Segment Personas, Salesforce DMP, Adobe Audience Manager, Oracle BlueKai, Lotame, Nielsen DMP, Comscore, MediaMath, The Trade Desk, Google Marketing Platform, Facebook Analytics, TikTok Analytics, Snapchat Analytics, Pinterest Analytics, LinkedIn Analytics, Twitter Analytics, Shopify Analytics, WooCommerce Analytics, Magento Analytics, BigCommerce Analytics, Salesforce Analytics, HubSpot Analytics, Marketo Analytics, Pardot Analytics, Eloqua Analytics, Zoho Analytics, Freshworks Analytics, ServiceNow Analytics, Jira Analytics, Asana Analytics, Trello Analytics, Monday.com Analytics, Smartsheet Analytics, Airtable Analytics, Notion Analytics, Coda Analytics, Quip Analytics, Confluence Analytics, Slack Analytics, Microsoft Teams Analytics, Zoom Analytics, Webex Analytics, Google Meet Analytics, Calendly Analytics, Acuity Scheduling Analytics, HubSpot CRM Analytics, Salesforce CRM Analytics, Zoho CRM Analytics, Freshsales Analytics, Pipedrive Analytics, Insightly Analytics, Nimble Analytics, Agile CRM Analytics, Keap Analytics, ActiveCampaign Analytics, Mailchimp Analytics, Constant Contact Analytics, Sendinblue Analytics, GetResponse Analytics, ConvertKit Analytics, AWeber Analytics, Campaign Monitor Analytics, Drip Analytics, Klaviyo Analytics, Omnisend Analytics, Attentive Analytics, Postscript Analytics, SMSBump Analytics, Yotpo Analytics, Judge.me Analytics, Loox Analytics, Stamped Analytics, Reviews.io Analytics, Trustpilot Analytics, G2 Analytics, Capterra Analytics, Gartner Analytics, Forrester Analytics, IDC Analytics, Nielsen Analytics, Comscore Analytics, Kantar Analytics, Ipsos Analytics, GfK Analytics, YouGov Analytics, Statista Analytics, SimilarWeb Analytics, SEMrush Analytics, Ahrefs Analytics, Moz Analytics, Majestic SEO Analytics, SpyFu Analytics, Keyword Planner, Google Search Console, Bing Webmaster Tools, Yandex Webmaster, Screaming Frog, DeepCrawl, Sitebulb, Botify, OnCrawl, Rank Ranger, SEO PowerSuite, Serpstat, Mangools, LongTail Pro, Keyword Tool, AnswerThePublic, Ubersuggest, BuzzSumo, ContentKing, ContentSquare, Glassbox, Decibel, FullStory, Hotjar, Lucky Orange, Mouseflow, SessionCam, Smartlook, Inspectlet, Crazy Egg, VWO Insights, AB Tasty, Optimizely, Google Analytics, Adobe Analytics, Mixpanel, Amplitude, Heap, Pendo, Countly, Matomo, Open Web Analytics, Piwik PRO, Fathom Analytics, Simple Analytics, Plausible Analytics, Umami, GoatCounter, Cloudflare Analytics, Fastly Insights, Akamai mPulse, Catchpoint, Dynatrace, New Relic, AppDynamics, Datadog, Splunk, Elasticsearch, Logstash, Kibana, Graylog, Sumo Logic, Loggly, Papertrail, Sentry, Rollbar, Bugsnag, Airbrake, Raygun, Instana, Honeycomb, Lightstep, SignalFx, Coralogix, Logz.io, Humio, Chronicle, Google Cloud Logging, AWS CloudTrail, Azure Monitor, IBM Cloud Monitoring, Oracle Cloud Monitoring, Alibaba Cloud Monitoring, DigitalOcean Monitoring, Heroku Metrics, Netlify Analytics, Vercel Analytics, Cloudflare Analytics, AWS Cost Explorer, Azure Cost Management, Google Cost Management, CloudHealth, Cloudability, Apptio, Kubecost, VMware vRealize, Turbonomic, Spot by NetApp, ParkMyCloud, CloudCheckr, Nutanix Beam, Azure Arc, AWS Systems Manager, Google Cloud Operations, Datadog, New Relic, AppDynamics, Dynatrace, Instana, Honeycomb, Lightstep, SignalFx, Coralogix, Logz.io, Splunk, Elasticsearch, Sumo Logic, Graylog, Loggly, Papertrail, Sentry, Rollbar, Bugsnag, Airbrake, Raygun, Instabug, Crashlytics, Firebase Crashlytics, Microsoft App Center, TestFlight, Google Play Console, Apple App Store Connect, ASO, Sensor Tower, App Annie, Appfigures, MobileAction, Priori Data, Apptopia, Adjust, AppsFlyer, Branch, Kochava, Singular, Tenjin, ironSource, Unity Analytics, GameAnalytics, DeltaDNA, PlayFab, Google Play Games, Apple Game Center, Steamworks, Epic Games Store, Xbox Live, PlayStation Network, Nintendo Switch, Twitch, Discord, YouTube Gaming, Facebook Gaming, Mixer, Caffeine, DLive, Trovo, Nimo TV, Bigo Live, HUYA, Douyu, Twitch Analytics, YouTube Analytics, Facebook Gaming Analytics, Mixer Analytics, Caffeine Analytics, DLive Analytics, Trovo Analytics, Nimo TV Analytics, Bigo Live Analytics, HUYA Analytics, Douyu Analytics, Steam Analytics, Epic Games Store Analytics, Xbox Analytics, PlayStation Analytics, Nintendo Analytics, Google Stadia Analytics, NVIDIA GeForce Now Analytics, Amazon Luna Analytics, Xbox Cloud Gaming Analytics, PlayStation Now Analytics, Shadow Analytics, Vortex Analytics, Parsec Analytics, Rainway Analytics, Moonlight Analytics, Steam Link Analytics, Remote Play Analytics, Chromecast Analytics, AirPlay Analytics, Miracast Analytics, DLNA Analytics, Plex Analytics, Emby Analytics, Jellyfin Analytics, Kodi Analytics, VLC Analytics, QuickTime Analytics, Windows Media Player Analytics, iTunes Analytics, Google Play Music Analytics, Spotify Analytics, Apple Music Analytics, Amazon Music Analytics, YouTube Music Analytics, Deezer Analytics, Tidal Analytics, Pandora Analytics, SoundCloud Analytics, Bandcamp Analytics, Audius Analytics, Resonate Analytics, Amuse Analytics, DistroKid Analytics, TuneCore Analytics, CD Baby Analytics, Ditto Music Analytics, Record Union Analytics, Symphonic Distribution Analytics, Believe Digital Analytics, ADA Worldwide Analytics, The Orchard Analytics, Ingrooves Analytics, Empire Distribution Analytics, Create Music Group Analytics, AWAL Analytics, Kobalt Analytics, Sony Music Analytics, Universal Music Group Analytics, Warner Music Group Analytics, BMG Analytics, Concord Analytics, [email protected] Analytics, Alpha Data Analytics, SoundScan Analytics, BuzzAngle Analytics, Midia Research Analytics, IFPI Analytics, RIAA Analytics, BPI Analytics, ARIA Analytics, BVMI Analytics, SNEP Analytics, PROMUSICAE Analytics, FIMI Analytics, AFP Analytics, GfK Entertainment Analytics, Official Charts Company Analytics, Billboard Analytics, Rolling Stone Analytics, Pitchfork Analytics, Metacritic Analytics, Album of the Year Analytics, RateYourMusic Analytics, Discogs Analytics, MusicBrainz Analytics, AllMusic Analytics, Genius Analytics, Shazam Analytics, SoundHound Analytics, Musixmatch Analytics, Spotify for Artists Analytics, Apple Music for Artists Analytics, YouTube Analytics for Artists, Amazon Music for Artists Analytics, Deezer for Creators Analytics, Tidal for Artists Analytics, SoundCloud for Artists Analytics, Bandcamp for Artists Analytics, Audius for Artists Analytics, Amuse for Artists Analytics, DistroKid for Artists Analytics, TuneCore for Artists Analytics, CD Baby for Artists Analytics, Ditto Music for Artists Analytics, Record Union for Artists Analytics, Symphonic Distribution for Artists Analytics, Believe Digital for Artists Analytics, ADA Worldwide for Artists Analytics, The Orchard for Artists Analytics, Ingrooves for Artists Analytics, Empire Distribution for Artists Analytics, Create Music Group for Artists Analytics, AWAL for Artists Analytics, Kobalt for Artists Analytics, Sony Music for Artists Analytics, Universal Music for Artists Analytics, Warner Music for Artists Analytics, BMG for Artists Analytics, Concord for Artists Analytics, [email protected] for Artists Analytics, Alpha Data for Artists Analytics, SoundScan for Artists Analytics, BuzzAngle for Artists Analytics, Midia Research for Artists Analytics, IFPI for Artists Analytics, RIAA for Artists Analytics, BPI for Artists Analytics, ARIA for Artists Analytics, BVMI for Artists Analytics, SNEP for Artists Analytics, PROMUSICAE for Artists Analytics, FIMI for Artists Analytics, AFP for Artists Analytics, GfK Entertainment for Artists Analytics, Official Charts Company for Artists Analytics, Billboard for Artists Analytics, Rolling Stone for Artists Analytics, Pitchfork for Artists Analytics, Metacritic for Artists Analytics, Album of the Year for Artists Analytics, RateYourMusic for Artists Analytics, Discogs for Artists Analytics, MusicBrainz for Artists Analytics, AllMusic for Artists Analytics, Genius for Artists Analytics, Shazam for Artists Analytics, SoundHound for Artists Analytics, Musixmatch for Artists Analytics, Spotify for Podcasters Analytics, Apple Podcasts Analytics, Google Podcasts Analytics, Anchor Analytics, Spreaker Analytics, Libsyn Analytics, Blubrry Analytics, Podbean Analytics, Buzzsprout Analytics, Transistor Analytics, Simplecast Analytics, Castos Analytics, Captivate Analytics, Podomatic Analytics, Audioboom Analytics, Megaphone Analytics, Omny Studio Analytics, Acast Analytics, Podcorn Analytics, Gumball Analytics, Giphy Analytics, Tenor Analytics, Imgur Analytics, Flickr Analytics, 500px Analytics, Unsplash Analytics, Pexels Analytics, Pixabay Analytics, Shutterstock Analytics, Getty Images Analytics, Adobe Stock Analytics, iStock Analytics, Dreamstime Analytics, Depositphotos Analytics, Canva Analytics, Figma Analytics, Sketch Analytics, Adobe XD Analytics, InVision Analytics, Marvel Analytics, Framer Analytics, Webflow Analytics, Bubble Analytics, Wix Analytics, Squarespace Analytics, Weebly Analytics, WordPress Analytics, Joomla Analytics, Drupal Analytics, Magento Analytics, Shopify Analytics, BigCommerce Analytics, WooCommerce Analytics, PrestaShop Analytics, OpenCart Analytics, Ecwid Analytics, Square Online Analytics, GoDaddy Analytics, Namecheap Analytics, Google Domains Analytics, Cloudflare Registrar Analytics, Hover Analytics, Gandi Analytics, OVHcloud Analytics, Hetzner Analytics, DigitalOcean Analytics, Linode Analytics, Vultr Analytics, AWS Lightsail Analytics, Google Cloud Platform Analytics, Microsoft Azure Analytics, IBM Cloud Analytics, Oracle Cloud Analytics, Alibaba Cloud Analytics\n",
            "\n",
            "Similaridade m√≠nima (0.0-1.0) [0.5]: 0.5\n",
            "\n",
            "Iniciando busca de vagas...\n",
            "Buscando no GitHub Jobs...\n",
            "Erro no GitHub Jobs: HTTPSConnectionPool(host='jobs.github.com', port=443): Max retries exceeded with url: /positions.json?description=sql+python+r (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ef5e8b66b10>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Buscando no Indeed...\n",
            "Buscando no Google...\n",
            "Nenhuma vaga encontrada\n",
            "Nenhuma vaga encontrada\n",
            "\n",
            "Busca conclu√≠da! 0 vagas encontradas.\n",
            "\n",
            "DICAS:\n",
            "- Para melhores resultados, use palavras-chave espec√≠ficas\n",
            "- Experimente com diferentes n√≠veis de similaridade\n",
            "- Combine palavras t√©cnicas com habilidades espec√≠ficas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Sistema de Busca de Vagas para S√£o Paulo\n",
        "Autor: Assistente AI\n",
        "Data: 2024\n",
        "\"\"\"\n",
        "\n",
        "print(\"üîß Configurando ambiente...\")\n",
        "\n",
        "# Instalar bibliotecas necess√°rias\n",
        "!pip install requests pandas beautifulsoup4 selenium webdriver-manager --quiet\n",
        "\n",
        "print(\"Ambiente configurado!\")\n",
        "print(\"Importando bibliotecas...\")\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# Configura√ß√µes\n",
        "OUTPUT_PATH = \"/content/sample_data/vagas_sp.xlsx\"\n",
        "DATA_INICIO = datetime(2024, 8, 1)  # 01/08/2024\n",
        "LOCALIZACAO = \"S√£o Paulo, SP\"\n",
        "\n",
        "class VagasSPFinder:\n",
        "    def __init__(self):\n",
        "        self.vagas_encontradas = []\n",
        "        self.total_vagas = 0\n",
        "        self.setup_selenium()\n",
        "\n",
        "    def setup_selenium(self):\n",
        "        \"\"\"Configura o driver do Selenium\"\"\"\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
        "\n",
        "        try:\n",
        "            self.driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
        "            self.driver.set_page_load_timeout(30)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao configurar Selenium: {e}\")\n",
        "            self.driver = None\n",
        "\n",
        "    def buscar_vagas_indeed(self):\n",
        "        \"\"\"Busca vagas no Indeed Brasil\"\"\"\n",
        "        print(\"Buscando vagas no Indeed Brasil...\")\n",
        "\n",
        "        try:\n",
        "            base_url = \"https://br.indeed.com/jobs\"\n",
        "            params = {\n",
        "                'q': 'tecnologia',\n",
        "                'l': LOCALIZACAO,\n",
        "                'fromage': '7',  # √öltimos 7 dias\n",
        "                'start': 0\n",
        "            }\n",
        "\n",
        "            for pagina in range(0, 3):  # Limitar a 3 p√°ginas\n",
        "                params['start'] = pagina * 10\n",
        "                response = requests.get(base_url, params=params, timeout=15)\n",
        "\n",
        "                if response.status_code != 200:\n",
        "                    print(f\"Erro no Indeed: {response.status_code}\")\n",
        "                    break\n",
        "\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                resultados = soup.find_all('div', class_='job_seen_beacon')\n",
        "\n",
        "                if not resultados:\n",
        "                    break\n",
        "\n",
        "                for vaga in resultados:\n",
        "                    try:\n",
        "                        titulo_elem = vaga.find('h2', class_='jobTitle')\n",
        "                        titulo = titulo_elem.text.strip() if titulo_elem else 'N/A'\n",
        "\n",
        "                        empresa_elem = vaga.find('span', class_='companyName')\n",
        "                        empresa = empresa_elem.text.strip() if empresa_elem else 'N/A'\n",
        "\n",
        "                        local_elem = vaga.find('div', class_='companyLocation')\n",
        "                        localizacao = local_elem.text.strip() if local_elem else LOCALIZACAO\n",
        "\n",
        "                        data_elem = vaga.find('span', class_='date')\n",
        "                        data_texto = data_elem.text.strip() if data_elem else ''\n",
        "\n",
        "                        # Converter texto de data para objeto datetime\n",
        "                        data_publicacao = self.parse_data_indeed(data_texto)\n",
        "\n",
        "                        if data_publicacao and data_publicacao >= DATA_INICIO:\n",
        "                            link_elem = vaga.find('a', class_='jcs-JobTitle')\n",
        "                            link = \"https://br.indeed.com\" + link_elem['href'] if link_elem else '#'\n",
        "\n",
        "                            self.vagas_encontradas.append({\n",
        "                                'titulo': titulo,\n",
        "                                'empresa': empresa,\n",
        "                                'localizacao': localizacao,\n",
        "                                'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                                'fonte': 'Indeed Brasil',\n",
        "                                'link': link,\n",
        "                                'descricao': 'Verifique no site'  # Descri√ß√£o n√£o dispon√≠vel na listagem\n",
        "                            })\n",
        "                            self.total_vagas += 1\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "                print(f\"P√°gina {pagina + 1} do Indeed processada. Total: {self.total_vagas} vagas\")\n",
        "                time.sleep(2)  # Respeitar o site\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao acessar Indeed: {e}\")\n",
        "\n",
        "    def parse_data_indeed(self, data_texto):\n",
        "        \"\"\"Converte o texto de data do Indeed para objeto datetime\"\"\"\n",
        "        try:\n",
        "            data_texto = data_texto.lower()\n",
        "            hoje = datetime.now()\n",
        "\n",
        "            if 'hoje' in data_texto or 'just' in data_texto:\n",
        "                return hoje\n",
        "            elif 'ontem' in data_texto:\n",
        "                return hoje - timedelta(days=1)\n",
        "            elif 'dias' in data_texto:\n",
        "                num_dias = int(re.search(r'(\\d+)', data_texto).group(1))\n",
        "                return hoje - timedelta(days=num_dias)\n",
        "            else:\n",
        "                # Tentar parse de data completa\n",
        "                try:\n",
        "                    return datetime.strptime(data_texto, '%d/%m/%Y')\n",
        "                except:\n",
        "                    return hoje  # Fallback para data atual\n",
        "        except:\n",
        "            return datetime.now()  # Fallback para data atual\n",
        "\n",
        "    def buscar_vagas_infojobs(self):\n",
        "        \"\"\"Busca vagas no InfoJobs\"\"\"\n",
        "        print(\"Buscando vagas no InfoJobs...\")\n",
        "\n",
        "        try:\n",
        "            base_url = \"https://www.infojobs.com.br/vagas-de-emprego\"\n",
        "            params = {\n",
        "                'palavraChave': 'tecnologia',\n",
        "                'cidade': 'S√£o Paulo',\n",
        "                'pagina': 1\n",
        "            }\n",
        "\n",
        "            for pagina in range(1, 4):  # Limitar a 3 p√°ginas\n",
        "                params['pagina'] = pagina\n",
        "                response = requests.get(base_url, params=params, timeout=15)\n",
        "\n",
        "                if response.status_code != 200:\n",
        "                    print(f\"Erro no InfoJobs: {response.status_code}\")\n",
        "                    break\n",
        "\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                resultados = soup.select('div.vaga')\n",
        "\n",
        "                if not resultados:\n",
        "                    break\n",
        "\n",
        "                for vaga in resultados:\n",
        "                    try:\n",
        "                        titulo_elem = vaga.select_one('h2.vagaTitle a')\n",
        "                        titulo = titulo_elem.text.strip() if titulo_elem else 'N/A'\n",
        "\n",
        "                        empresa_elem = vaga.select_one('span.vaga-company')\n",
        "                        empresa = empresa_elem.text.strip() if empresa_elem else 'N/A'\n",
        "\n",
        "                        local_elem = vaga.select_one('span.vaga-location')\n",
        "                        localizacao = local_elem.text.strip() if local_elem else LOCALIZACAO\n",
        "\n",
        "                        data_elem = vaga.select_one('span.vaga-date')\n",
        "                        data_texto = data_elem.text.strip() if data_elem else ''\n",
        "\n",
        "                        # Converter texto de data para objeto datetime\n",
        "                        data_publicacao = self.parse_data_infojobs(data_texto)\n",
        "\n",
        "                        if data_publicacao and data_publicacao >= DATA_INICIO:\n",
        "                            link = titulo_elem['href'] if titulo_elem else '#'\n",
        "                            if not link.startswith('http'):\n",
        "                                link = 'https://www.infojobs.com.br' + link\n",
        "\n",
        "                            self.vagas_encontradas.append({\n",
        "                                'titulo': titulo,\n",
        "                                'empresa': empresa,\n",
        "                                'localizacao': localizacao,\n",
        "                                'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                                'fonte': 'InfoJobs',\n",
        "                                'link': link,\n",
        "                                'descricao': 'Verifique no site'\n",
        "                            })\n",
        "                            self.total_vagas += 1\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "                print(f\"P√°gina {pagina} do InfoJobs processada. Total: {self.total_vagas} vagas\")\n",
        "                time.sleep(2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao acessar InfoJobs: {e}\")\n",
        "\n",
        "    def parse_data_infojobs(self, data_texto):\n",
        "        \"\"\"Converte o texto de data do InfoJobs para objeto datetime\"\"\"\n",
        "        try:\n",
        "            # Formato comum: \"Publicada em 15/08/2024\"\n",
        "            match = re.search(r'(\\d{2}/\\d{2}/\\d{4})', data_texto)\n",
        "            if match:\n",
        "                return datetime.strptime(match.group(1), '%d/%m/%Y')\n",
        "\n",
        "            # Outros formatos com dias relativos\n",
        "            data_texto = data_texto.lower()\n",
        "            hoje = datetime.now()\n",
        "\n",
        "            if 'hoje' in data_texto:\n",
        "                return hoje\n",
        "            elif 'ontem' in data_texto:\n",
        "                return hoje - timedelta(days=1)\n",
        "            elif 'dias' in data_texto:\n",
        "                num_dias = int(re.search(r'(\\d+)', data_texto).group(1))\n",
        "                return hoje - timedelta(days=num_dias)\n",
        "            else:\n",
        "                return hoje  # Fallback\n",
        "        except:\n",
        "            return datetime.now()  # Fallback\n",
        "\n",
        "    def buscar_vagas_linkedin(self):\n",
        "        \"\"\"Tenta buscar vagas no LinkedIn (usando Selenium)\"\"\"\n",
        "        if not self.driver:\n",
        "            print(\"Driver do Selenium n√£o dispon√≠vel para LinkedIn\")\n",
        "            return\n",
        "\n",
        "        print(\"Buscando vagas no LinkedIn...\")\n",
        "\n",
        "        try:\n",
        "            # URL de busca do LinkedIn para tecnologia em S√£o Paulo\n",
        "            url = f\"https://www.linkedin.com/jobs/search/?keywords=tecnologia&location={LOCALIZACAO}&f_TPR=r604800&position=1&pageNum=0\"\n",
        "\n",
        "            self.driver.get(url)\n",
        "            time.sleep(5)  # Esperar carregar\n",
        "\n",
        "            # Tentar encontrar elementos de vagas\n",
        "            vagas = self.driver.find_elements(By.CSS_SELECTOR, '.jobs-search__results-list li')\n",
        "\n",
        "            for i, vaga in enumerate(vagas[:10]):  # Limitar a 10 vagas\n",
        "                try:\n",
        "                    # Extrair informa√ß√µes sem clicar\n",
        "                    titulo = vaga.find_element(By.CSS_SELECTOR, '.base-search-card__title').text\n",
        "                    empresa = vaga.find_element(By.CSS_SELECTOR, '.base-search-card__subtitle').text\n",
        "                    localizacao = vaga.find_element(By.CSS_SELECTOR, '.job-search-card__location').text\n",
        "\n",
        "                    # A data n√£o √© facilmente acess√≠vel, usar data atual como aproxima√ß√£o\n",
        "                    data_publicacao = datetime.now()\n",
        "\n",
        "                    if data_publicacao >= DATA_INICIO:\n",
        "                        link = vaga.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')\n",
        "\n",
        "                        self.vagas_encontradas.append({\n",
        "                            'titulo': titulo,\n",
        "                            'empresa': empresa,\n",
        "                            'localizacao': localizacao,\n",
        "                            'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                            'fonte': 'LinkedIn',\n",
        "                            'link': link,\n",
        "                            'descricao': 'Verifique no site'\n",
        "                        })\n",
        "                        self.total_vagas += 1\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "                if i >= 9:  # Parar ap√≥s 10 vagas\n",
        "                    break\n",
        "\n",
        "            print(f\"LinkedIn processado. Total: {self.total_vagas} vagas\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao acessar LinkedIn: {e}\")\n",
        "\n",
        "    def buscar_vagas_brasil(self):\n",
        "        \"\"\"Busca em sites de empregos brasileiros alternativos\"\"\"\n",
        "        print(\"Buscando em sites brasileiros alternativos...\")\n",
        "\n",
        "        # Podemos adicionar mais sites brasileiros aqui\n",
        "        sites = [\n",
        "            {\n",
        "                'nome': 'Vagas.com',\n",
        "                'url': 'https://www.vagas.com.br/vagas-de-tecnologia-em-sao-paulo',\n",
        "                'selector': '.vaga',\n",
        "                'titulo_selector': '.cargo',\n",
        "                'empresa_selector': '.empr',\n",
        "                'local_selector': '.vaga-local',\n",
        "                'data_selector': '.data'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        for site in sites:\n",
        "            try:\n",
        "                response = requests.get(site['url'], timeout=15)\n",
        "                if response.status_code != 200:\n",
        "                    continue\n",
        "\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                vagas = soup.select(site['selector'])\n",
        "\n",
        "                for vaga in vagas[:5]:  # Limitar a 5 vagas por site\n",
        "                    try:\n",
        "                        titulo = vaga.select_one(site['titulo_selector']).text.strip() if vaga.select_one(site['titulo_selector']) else 'N/A'\n",
        "                        empresa = vaga.select_one(site['empresa_selector']).text.strip() if vaga.select_one(site['empresa_selector']) else 'N/A'\n",
        "                        localizacao = vaga.select_one(site['local_selector']).text.strip() if vaga.select_one(site['local_selector']) else LOCALIZACAO\n",
        "\n",
        "                        # Usar data atual como fallback\n",
        "                        data_publicacao = datetime.now()\n",
        "\n",
        "                        self.vagas_encontradas.append({\n",
        "                            'titulo': titulo,\n",
        "                            'empresa': empresa,\n",
        "                            'localizacao': localizacao,\n",
        "                            'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                            'fonte': site['nome'],\n",
        "                            'link': site['url'],\n",
        "                            'descricao': 'Verifique no site'\n",
        "                        })\n",
        "                        self.total_vagas += 1\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                print(f\"{site['nome']} processado. Total: {self.total_vagas} vagas\")\n",
        "                time.sleep(2)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao acessar {site['nome']}: {e}\")\n",
        "                continue\n",
        "\n",
        "    def buscar_todas_vagas(self):\n",
        "        \"\"\"Executa todas as buscas\"\"\"\n",
        "        print(\"Iniciando busca por vagas em S√£o Paulo...\")\n",
        "        print(f\"Per√≠odo: desde {DATA_INICIO.strftime('%d/%m/%Y')} at√© hoje\")\n",
        "        print(f\"Localiza√ß√£o: {LOCALIZACAO}\")\n",
        "\n",
        "        self.buscar_vagas_indeed()\n",
        "        self.buscar_vagas_infojobs()\n",
        "        self.buscar_vagas_linkedin()\n",
        "        self.buscar_vagas_brasil()\n",
        "\n",
        "        print(f\"Busca conclu√≠da! Total de {self.total_vagas} vagas encontradas.\")\n",
        "\n",
        "    def exportar_resultados(self):\n",
        "        \"\"\"Exporta os resultados para Excel\"\"\"\n",
        "        if not self.vagas_encontradas:\n",
        "            print(\"Nenhuma vaga encontrada para exportar\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(self.vagas_encontradas)\n",
        "\n",
        "        # Reordenar colunas\n",
        "        colunas = ['titulo', 'empresa', 'localizacao', 'data_publicacao', 'fonte', 'descricao', 'link']\n",
        "        df = df[colunas]\n",
        "\n",
        "        df.to_excel(OUTPUT_PATH, index=False)\n",
        "        print(f\"Resultados exportados para: {OUTPUT_PATH}\")\n",
        "\n",
        "    def mostrar_resultados(self):\n",
        "        \"\"\"Mostra os resultados na tela\"\"\"\n",
        "        if not self.vagas_encontradas:\n",
        "            print(\"Nenhuma vaga encontrada\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nVAGAS ENCONTRADAS: {self.total_vagas}\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        for i, vaga in enumerate(self.vagas_encontradas[:15], 1):  # Mostrar apenas as 15 primeiras\n",
        "            print(f\"\\n{i}. {vaga['titulo']}\")\n",
        "            print(f\"   {vaga['empresa']}\")\n",
        "            print(f\"   {vaga['localizacao']}\")\n",
        "            print(f\"   {vaga['data_publicacao']}\")\n",
        "            print(f\"   {vaga['fonte']}\")\n",
        "            print(f\"   {vaga['link']}\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "        if self.total_vagas > 15:\n",
        "            print(f\"\\n... e mais {self.total_vagas - 15} vagas (ver arquivo Excel para lista completa)\")\n",
        "\n",
        "# Executar a busca\n",
        "if __name__ == \"__main__\":\n",
        "    finder = VagasSPFinder()\n",
        "    finder.buscar_todas_vagas()\n",
        "    finder.mostrar_resultados()\n",
        "    finder.exportar_resultados()\n",
        "\n",
        "    # Fechar o driver do Selenium se existir\n",
        "    if hasattr(finder, 'driver') and finder.driver:\n",
        "        finder.driver.quit()"
      ],
      "metadata": {
        "id": "RaRboHlpq8TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Sistema Avan√ßado de Busca de Vagas com Palavras-Chave\n",
        "Autor: Assistente AI\n",
        "Data: 2025\n",
        "\"\"\"\n",
        "\n",
        "print(\"üîß Configurando ambiente...\")\n",
        "\n",
        "# Instalar bibliotecas necess√°rias\n",
        "!pip install requests pandas beautifulsoup4 selenium webdriver-manager google-search-results --quiet\n",
        "\n",
        "print(\"Ambiente configurado!\")\n",
        "print(\"Importando bibliotecas...\")\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from serpapi import GoogleSearch\n",
        "\n",
        "# Configura√ß√µes\n",
        "OUTPUT_PATH = \"/content/sample_data/vagas_palavras_chave.xlsx\"\n",
        "DATA_INICIO = datetime(2024, 8, 1)  # 01/08/2024\n",
        "PALAVRAS_CHAVE = [\"python\", \"desenvolvedor\", \"tecnologia\", \"analista\", \"programador\"]\n",
        "LOCALIZACAO = \"S√£o Paulo, SP\"\n",
        "\n",
        "class VagasAvancadoFinder:\n",
        "    def __init__(self, palavras_chave=None):\n",
        "        self.vagas_encontradas = []\n",
        "        self.total_vagas = 0\n",
        "        self.palavras_chave = palavras_chave or [\"tecnologia\"]\n",
        "        self.setup_selenium()\n",
        "\n",
        "    def setup_selenium(self):\n",
        "        \"\"\"Configura o driver do Selenium\"\"\"\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
        "\n",
        "        try:\n",
        "            self.driver = webdriver.Chrome(\n",
        "                ChromeDriverManager().install(),\n",
        "                options=chrome_options\n",
        "            )\n",
        "            self.driver.set_page_load_timeout(30)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao configurar Selenium: {e}\")\n",
        "            self.driver = None\n",
        "\n",
        "    def buscar_vagas_google_jobs(self):\n",
        "        \"\"\"Busca vagas usando Google Jobs API atrav√©s do SerpApi\"\"\"\n",
        "        print(\"Buscando vagas no Google Jobs...\")\n",
        "\n",
        "        try:\n",
        "            # Configura√ß√£o da API (substitua pela sua chave SerpApi)\n",
        "            params = {\n",
        "                \"engine\": \"google_jobs\",\n",
        "                \"q\": f\"{' '.join(self.palavras_chave)} {LOCALIZACAO}\",\n",
        "                \"hl\": \"pt\",\n",
        "                \"gl\": \"br\",\n",
        "                \"api_key\": \"SUA_CHAVE_SERPAPI_AQUI\"  # Obtenha em: https://serpapi.com/manage-api-key\n",
        "            }\n",
        "\n",
        "            search = GoogleSearch(params)\n",
        "            results = search.get_dict()\n",
        "\n",
        "            if \"jobs_results\" in results:\n",
        "                for vaga in results[\"jobs_results\"]:\n",
        "                    try:\n",
        "                        # Converter data para objeto datetime\n",
        "                        data_texto = vaga.get(\"detected_extensions\", {}).get(\"posted_at\", \"\")\n",
        "                        data_publicacao = self.parse_data_google_jobs(data_texto)\n",
        "\n",
        "                        if data_publicacao >= DATA_INICIO:\n",
        "                            self.vagas_encontradas.append({\n",
        "                                'titulo': vaga.get('title', 'N/A'),\n",
        "                                'empresa': vaga.get('company_name', 'N/A'),\n",
        "                                'localizacao': vaga.get('location', LOCALIZACAO),\n",
        "                                'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                                'fonte': 'Google Jobs',\n",
        "                                'link': vaga.get('related_links', [{}])[0].get('link', '#') if vaga.get('related_links') else '#',\n",
        "                                'descricao': vaga.get('description', '')[:100] + '...' if vaga.get('description') else 'N/A'\n",
        "                            })\n",
        "                            self.total_vagas += 1\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "                print(f\"Google Jobs processado. Total: {self.total_vagas} vagas\")\n",
        "            else:\n",
        "                print(\"Nenhum resultado encontrado no Google Jobs\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao acessar Google Jobs API: {e}\")\n",
        "\n",
        "    def parse_data_google_jobs(self, data_texto):\n",
        "        \"\"\"Converte o texto de data do Google Jobs para objeto datetime\"\"\"\n",
        "        try:\n",
        "            data_texto = data_texto.lower()\n",
        "            hoje = datetime.now()\n",
        "\n",
        "            if 'hour' in data_texto or 'hora' in data_texto:\n",
        "                return hoje\n",
        "            elif 'day' in data_texto or 'dia' in data_texto:\n",
        "                num_dias = int(re.search(r'(\\d+)', data_texto).group(1))\n",
        "                return hoje - timedelta(days=num_dias)\n",
        "            elif 'week' in data_texto or 'semana' in data_texto:\n",
        "                num_semanas = int(re.search(r'(\\d+)', data_texto).group(1))\n",
        "                return hoje - timedelta(weeks=num_semanas)\n",
        "            else:\n",
        "                return hoje  # Fallback para data atual\n",
        "        except:\n",
        "            return datetime.now()  # Fallback para data atual\n",
        "\n",
        "    def buscar_vagas_linkedin(self):\n",
        "        \"\"\"Busca vagas no LinkedIn usando scraping\"\"\"\n",
        "        if not self.driver:\n",
        "            print(\"Driver do Selenium n√£o dispon√≠vel para LinkedIn\")\n",
        "            return\n",
        "\n",
        "        print(\"Buscando vagas no LinkedIn...\")\n",
        "\n",
        "        try:\n",
        "            # Construir URL de busca com palavras-chave\n",
        "            keywords_encoded = \"%20\".join(self.palavras_chave)\n",
        "            location_encoded = LOCALIZACAO.replace(\" \", \"%20\").replace(\",\", \"%2C\")\n",
        "\n",
        "            url = f\"https://www.linkedin.com/jobs/search/?keywords={keywords_encoded}&location={location_encoded}&f_TPR=r2592000\"\n",
        "\n",
        "            self.driver.get(url)\n",
        "            time.sleep(5)  # Esperar carregar\n",
        "\n",
        "            # Scroll para carregar mais vagas\n",
        "            for _ in range(3):\n",
        "                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "                time.sleep(2)\n",
        "\n",
        "            # Coletar elementos de vagas\n",
        "            vagas = self.driver.find_elements(By.CSS_SELECTOR, '.jobs-search__results-list li')\n",
        "\n",
        "            for i, vaga in enumerate(vagas[:20]):  # Limitar a 20 vagas\n",
        "                try:\n",
        "                    titulo = vaga.find_element(By.CSS_SELECTOR, '.base-search-card__title').text\n",
        "                    empresa = vaga.find_element(By.CSS_SELECTOR, '.base-search-card__subtitle').text\n",
        "                    localizacao = vaga.find_element(By.CSS_SELECTOR, '.job-search-card__location').text\n",
        "                    link = vaga.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')\n",
        "\n",
        "                    # A data n√£o √© facilmente acess√≠vel, usar data atual como aproxima√ß√£o\n",
        "                    data_publicacao = datetime.now()\n",
        "\n",
        "                    if data_publicacao >= DATA_INICIO:\n",
        "                        self.vagas_encontradas.append({\n",
        "                            'titulo': titulo,\n",
        "                            'empresa': empresa,\n",
        "                            'localizacao': localizacao,\n",
        "                            'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                            'fonte': 'LinkedIn',\n",
        "                            'link': link,\n",
        "                            'descricao': 'Verifique no site para detalhes completos'\n",
        "                        })\n",
        "                        self.total_vagas += 1\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "            print(f\"LinkedIn processado. Total: {self.total_vagas} vagas\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao acessar LinkedIn: {e}\")\n",
        "\n",
        "    def buscar_vagas_indeed(self):\n",
        "        \"\"\"Busca vagas no Indeed Brasil\"\"\"\n",
        "        print(\"Buscando vagas no Indeed Brasil...\")\n",
        "\n",
        "        try:\n",
        "            base_url = \"https://br.indeed.com/jobs\"\n",
        "\n",
        "            for palavra_chave in self.palavras_chave:\n",
        "                params = {\n",
        "                    'q': palavra_chave,\n",
        "                    'l': LOCALIZACAO,\n",
        "                    'fromage': '7',  # √öltimos 7 dias\n",
        "                    'start': 0\n",
        "                }\n",
        "\n",
        "                for pagina in range(0, 2):  # Limitar a 2 p√°ginas por palavra-chave\n",
        "                    params['start'] = pagina * 10\n",
        "                    headers = {\n",
        "                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "                    }\n",
        "\n",
        "                    response = requests.get(base_url, params=params, headers=headers, timeout=15)\n",
        "\n",
        "                    if response.status_code != 200:\n",
        "                        print(f\"Erro no Indeed: {response.status_code}\")\n",
        "                        break\n",
        "\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                    resultados = soup.find_all('div', class_='job_seen_beacon')\n",
        "\n",
        "                    if not resultados:\n",
        "                        break\n",
        "\n",
        "                    for vaga in resultados:\n",
        "                        try:\n",
        "                            titulo_elem = vaga.find('h2', class_='jobTitle')\n",
        "                            titulo = titulo_elem.text.strip() if titulo_elem else 'N/A'\n",
        "\n",
        "                            empresa_elem = vaga.find('span', class_='companyName')\n",
        "                            empresa = empresa_elem.text.strip() if empresa_elem else 'N/A'\n",
        "\n",
        "                            local_elem = vaga.find('div', class_='companyLocation')\n",
        "                            localizacao = local_elem.text.strip() if local_elem else LOCALIZACAO\n",
        "\n",
        "                            data_elem = vaga.find('span', class_='date')\n",
        "                            data_texto = data_elem.text.strip() if data_elem else ''\n",
        "\n",
        "                            # Converter texto de data para objeto datetime\n",
        "                            data_publicacao = self.parse_data_indeed(data_texto)\n",
        "\n",
        "                            if data_publicacao >= DATA_INICIO:\n",
        "                                link_elem = vaga.find('a', class_='jcs-JobTitle')\n",
        "                                link = \"https://br.indeed.com\" + link_elem['href'] if link_elem else '#'\n",
        "\n",
        "                                self.vagas_encontradas.append({\n",
        "                                    'titulo': titulo,\n",
        "                                    'empresa': empresa,\n",
        "                                    'localizacao': localizacao,\n",
        "                                    'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                                    'fonte': 'Indeed Brasil',\n",
        "                                    'link': link,\n",
        "                                    'descricao': 'Verifique no site para detalhes completos'\n",
        "                                })\n",
        "                                self.total_vagas += 1\n",
        "                        except Exception as e:\n",
        "                            continue\n",
        "\n",
        "                    print(f\"Palavra-chave '{palavra_chave}' - P√°gina {pagina + 1} do Indeed processada. Total: {self.total_vagas} vagas\")\n",
        "                    time.sleep(2)  # Respeitar o site\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao acessar Indeed: {e}\")\n",
        "\n",
        "    def parse_data_indeed(self, data_texto):\n",
        "        \"\"\"Converte o texto de data do Indeed para objeto datetime\"\"\"\n",
        "        try:\n",
        "            data_texto = data_texto.lower()\n",
        "            hoje = datetime.now()\n",
        "\n",
        "            if 'hoje' in data_texto or 'just' in data_texto:\n",
        "                return hoje\n",
        "            elif 'ontem' in data_texto:\n",
        "                return hoje - timedelta(days=1)\n",
        "            elif 'dias' in data_texto:\n",
        "                num_dias = int(re.search(r'(\\d+)', data_texto).group(1))\n",
        "                return hoje - timedelta(days=num_dias)\n",
        "            else:\n",
        "                # Tentar parse de data completa\n",
        "                try:\n",
        "                    return datetime.strptime(data_texto, '%d/%m/%Y')\n",
        "                except:\n",
        "                    return hoje  # Fallback para data atual\n",
        "        except:\n",
        "            return datetime.now()  # Fallback para data atual\n",
        "\n",
        "    def buscar_vagas_infojobs(self):\n",
        "        \"\"\"Busca vagas no InfoJobs\"\"\"\n",
        "        print(\"üîç Buscando vagas no InfoJobs...\")\n",
        "\n",
        "        try:\n",
        "            base_url = \"https://www.infojobs.com.br/vagas-de-emprego\"\n",
        "\n",
        "            for palavra_chave in self.palavras_chave:\n",
        "                params = {\n",
        "                    'palavraChave': palavra_chave,\n",
        "                    'cidade': 'S√£o Paulo',\n",
        "                    'pagina': 1\n",
        "                }\n",
        "\n",
        "                for pagina in range(1, 3):  # Limitar a 2 p√°ginas por palavra-chave\n",
        "                    params['pagina'] = pagina\n",
        "                    headers = {\n",
        "                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "                    }\n",
        "\n",
        "                    response = requests.get(base_url, params=params, headers=headers, timeout=15)\n",
        "\n",
        "                    if response.status_code != 200:\n",
        "                        print(f\"Erro no InfoJobs: {response.status_code}\")\n",
        "                        break\n",
        "\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                    resultados = soup.select('div.vaga')\n",
        "\n",
        "                    if not resultados:\n",
        "                        break\n",
        "\n",
        "                    for vaga in resultados:\n",
        "                        try:\n",
        "                            titulo_elem = vaga.select_one('h2.vagaTitle a')\n",
        "                            titulo = titulo_elem.text.strip() if titulo_elem else 'N/A'\n",
        "\n",
        "                            empresa_elem = vaga.select_one('span.vaga-company')\n",
        "                            empresa = empresa_elem.text.strip() if empresa_elem else 'N/A'\n",
        "\n",
        "                            local_elem = vaga.select_one('span.vaga-location')\n",
        "                            localizacao = local_elem.text.strip() if local_elem else LOCALIZACAO\n",
        "\n",
        "                            data_elem = vaga.select_one('span.vaga-date')\n",
        "                            data_texto = data_elem.text.strip() if data_elem else ''\n",
        "\n",
        "                            # Converter texto de data para objeto datetime\n",
        "                            data_publicacao = self.parse_data_infojobs(data_texto)\n",
        "\n",
        "                            if data_publicacao >= DATA_INICIO:\n",
        "                                link = titulo_elem['href'] if titulo_elem else '#'\n",
        "                                if not link.startswith('http'):\n",
        "                                    link = 'https://www.infojobs.com.br' + link\n",
        "\n",
        "                                self.vagas_encontradas.append({\n",
        "                                    'titulo': titulo,\n",
        "                                    'empresa': empresa,\n",
        "                                    'localizacao': localizacao,\n",
        "                                    'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                                    'fonte': 'InfoJobs',\n",
        "                                    'link': link,\n",
        "                                    'descricao': 'Verifique no site para detalhes completos'\n",
        "                                })\n",
        "                                self.total_vagas += 1\n",
        "                        except Exception as e:\n",
        "                            continue\n",
        "\n",
        "                    print(f\"Palavra-chave '{palavra_chave}' - P√°gina {pagina} do InfoJobs processada. Total: {self.total_vagas} vagas\")\n",
        "                    time.sleep(2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao acessar InfoJobs: {e}\")\n",
        "\n",
        "    def parse_data_infojobs(self, data_texto):\n",
        "        \"\"\"Converte o texto de data do InfoJobs para objeto datetime\"\"\"\n",
        "        try:\n",
        "            # Formato comum: \"Publicada em 15/08/2024\"\n",
        "            match = re.search(r'(\\d{2}/\\d{2}/\\d{4})', data_texto)\n",
        "            if match:\n",
        "                return datetime.strptime(match.group(1), '%d/%m/%Y')\n",
        "\n",
        "            # Outros formatos com dias relativos\n",
        "            data_texto = data_texto.lower()\n",
        "            hoje = datetime.now()\n",
        "\n",
        "            if 'hoje' in data_texto:\n",
        "                return hoje\n",
        "            elif 'ontem' in data_texto:\n",
        "                return hoje - timedelta(days=1)\n",
        "            elif 'dias' in data_texto:\n",
        "                num_dias = int(re.search(r'(\\d+)', data_texto).group(1))\n",
        "                return hoje - timedelta(days=num_dias)\n",
        "            else:\n",
        "                return hoje  # Fallback\n",
        "        except:\n",
        "            return datetime.now()  # Fallback\n",
        "\n",
        "    def buscar_vagas_brasil(self):\n",
        "        \"\"\"Busca em sites de empregos brasileiros alternativos\"\"\"\n",
        "        print(\"Buscando em sites brasileiros alternativos...\")\n",
        "\n",
        "        # Lista de sites para buscar\n",
        "        sites = [\n",
        "            {\n",
        "                'nome': 'Vagas.com',\n",
        "                'url': f\"https://www.vagas.com.br/vagas-de-{self.palavras_chave[0]}-em-sao-paulo\",\n",
        "                'selector': '.vaga',\n",
        "                'titulo_selector': '.cargo',\n",
        "                'empresa_selector': '.empr',\n",
        "                'local_selector': '.vaga-local',\n",
        "                'data_selector': '.data'\n",
        "            },\n",
        "            {\n",
        "                'nome': 'Catho',\n",
        "                'url': f\"https://www.catho.com.br/vagas/{self.palavras_chave[0]}/em-sao-paulo-sp/\",\n",
        "                'selector': '.job-card',\n",
        "                'titulo_selector': '.job-card__title',\n",
        "                'empresa_selector': '.job-card__company',\n",
        "                'local_selector': '.job-card__location',\n",
        "                'data_selector': '.job-card__date'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        for site in sites:\n",
        "            try:\n",
        "                headers = {\n",
        "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "                }\n",
        "\n",
        "                response = requests.get(site['url'], headers=headers, timeout=15)\n",
        "                if response.status_code != 200:\n",
        "                    continue\n",
        "\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                vagas = soup.select(site['selector'])\n",
        "\n",
        "                for vaga in vagas[:5]:  # Limitar a 5 vagas por site\n",
        "                    try:\n",
        "                        titulo = vaga.select_one(site['titulo_selector']).text.strip() if vaga.select_one(site['titulo_selector']) else 'N/A'\n",
        "                        empresa = vaga.select_one(site['empresa_selector']).text.strip() if vaga.select_one(site['empresa_selector']) else 'N/A'\n",
        "                        localizacao = vaga.select_one(site['local_selector']).text.strip() if vaga.select_one(site['local_selector']) else LOCALIZACAO\n",
        "\n",
        "                        # Usar data atual como fallback\n",
        "                        data_publicacao = datetime.now()\n",
        "\n",
        "                        self.vagas_encontradas.append({\n",
        "                            'titulo': titulo,\n",
        "                            'empresa': empresa,\n",
        "                            'localizacao': localizacao,\n",
        "                            'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                            'fonte': site['nome'],\n",
        "                            'link': site['url'],\n",
        "                            'descricao': 'Verifique no site para detalhes completos'\n",
        "                        })\n",
        "                        self.total_vagas += 1\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                print(f\"{site['nome']} processado. Total: {self.total_vagas} vagas\")\n",
        "                time.sleep(2)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao acessar {site['nome']}: {e}\")\n",
        "                continue\n",
        "\n",
        "    def remover_duplicatas(self):\n",
        "        \"\"\"Remove vagas duplicadas com base no t√≠tulo e empresa\"\"\"\n",
        "        df = pd.DataFrame(self.vagas_encontradas)\n",
        "        if not df.empty:\n",
        "            df = df.drop_duplicates(subset=['titulo', 'empresa'], keep='first')\n",
        "            self.vagas_encontradas = df.to_dict('records')\n",
        "            self.total_vagas = len(self.vagas_encontradas)\n",
        "\n",
        "    def buscar_todas_vagas(self):\n",
        "        \"\"\"Executa todas as buscas\"\"\"\n",
        "        print(\"Iniciando busca por vagas em S√£o Paulo...\")\n",
        "        print(f\"Per√≠odo: desde {DATA_INICIO.strftime('%d/%m/%Y')} at√© hoje\")\n",
        "        print(f\"Localiza√ß√£o: {LOCALIZACAO}\")\n",
        "        print(f\"Palavras-chave: {', '.join(self.palavras_chave)}\")\n",
        "\n",
        "        # Executar todas as buscas\n",
        "        self.buscar_vagas_google_jobs()\n",
        "        self.buscar_vagas_linkedin()\n",
        "        self.buscar_vagas_indeed()\n",
        "        self.buscar_vagas_infojobs()\n",
        "        self.buscar_vagas_brasil()\n",
        "\n",
        "        # Remover duplicatas\n",
        "        self.remover_duplicatas()\n",
        "\n",
        "        print(f\"Busca conclu√≠da! Total de {self.total_vagas} vagas encontradas.\")\n",
        "\n",
        "    def exportar_resultados(self):\n",
        "        \"\"\"Exporta os resultados para Excel\"\"\"\n",
        "        if not self.vagas_encontradas:\n",
        "            print(\"Nenhuma vaga encontrada para exportar\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(self.vagas_encontradas)\n",
        "\n",
        "        # Reordenar colunas\n",
        "        colunas = ['titulo', 'empresa', 'localizacao', 'data_publicacao', 'fonte', 'descricao', 'link']\n",
        "        df = df[colunas]\n",
        "\n",
        "        df.to_excel(OUTPUT_PATH, index=False)\n",
        "        print(f\"Resultados exportados para: {OUTPUT_PATH}\")\n",
        "\n",
        "    def mostrar_resultados(self):\n",
        "        \"\"\"Mostra os resultados na tela\"\"\"\n",
        "        if not self.vagas_encontradas:\n",
        "            print(\"Nenhuma vaga encontrada\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nVAGAS ENCONTRADAS: {self.total_vagas}\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        for i, vaga in enumerate(self.vagas_encontradas[:20], 1):  # Mostrar apenas as 20 primeiras\n",
        "            print(f\"\\n{i}. {vaga['titulo']}\")\n",
        "            print(f\"   {vaga['empresa']}\")\n",
        "            print(f\"   {vaga['localizacao']}\")\n",
        "            print(f\"   {vaga['data_publicacao']}\")\n",
        "            print(f\"   {vaga['fonte']}\")\n",
        "            print(f\"   {vaga['link']}\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "        if self.total_vagas > 20:\n",
        "            print(f\"\\n... e mais {self.total_vagas - 20} vagas (ver arquivo Excel para lista completa)\")\n",
        "\n",
        "# Executar a busca\n",
        "if __name__ == \"__main__\":\n",
        "    # Palavras-chave personalizadas\n",
        "    palavras_chave = input(\"Digite as palavras-chave para busca (separadas por v√≠rgula): \").split(',')\n",
        "    palavras_chave = [p.strip() for p in palavras_chave if p.strip()]\n",
        "\n",
        "    if not palavras_chave:\n",
        "        palavras_chave = PALAVRAS_CHAVE\n",
        "\n",
        "    finder = VagasAvancadoFinder(palavras_chave=palavras_chave)\n",
        "    finder.buscar_todas_vagas()\n",
        "    finder.mostrar_resultados()\n",
        "    finder.exportar_resultados()\n",
        "\n",
        "    # Fechar o driver do Selenium se existir\n",
        "    if hasattr(finder, 'driver') and finder.driver:\n",
        "        finder.driver.quit()"
      ],
      "metadata": {
        "id": "i-acbFl37PyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Sistema Inteligente de Busca de Vagas para √Årea de Dados\n",
        "Autor: Assistente AI\n",
        "Data: 2025\n",
        "\"\"\"\n",
        "\n",
        "print(\"üîß Configurando ambiente...\")\n",
        "\n",
        "# Instalar bibliotecas necess√°rias\n",
        "!pip install requests pandas beautifulsoup4 selenium webdriver-manager --quiet\n",
        "\n",
        "print(\"Ambiente configurado!\")\n",
        "print(\"Importando bibliotecas...\")\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import hashlib\n",
        "\n",
        "# Configura√ß√µes\n",
        "OUTPUT_PATH = \"/content/sample_data/vagas_dados_otimizado.xlsx\"\n",
        "DATA_INICIO = datetime(2024, 8, 1)  # 01/08/2024\n",
        "LOCALIZACAO = \"S√£o Paulo, SP\"\n",
        "\n",
        "# Palavras-chave priorit√°rias para √°rea de dados\n",
        "PALAVRAS_CHAVE = [\n",
        "    \"analista de dados\", \"data analyst\", \"cientista de dados\", \"data scientist\",\n",
        "    \"sql\", \"python\", \"business intelligence\", \"preditivo\", \"predictive\",\n",
        "    \"modelagem\", \"etl\", \"Manipula√ß√£o de Dados\", \"Excel\", \"Big Data\",\n",
        "    \"Machine Learning\", \"estat√≠stica\", \"power bi\", \"tableau\", \"data warehouse\"\n",
        "]\n",
        "\n",
        "class VagasInteligenteFinder:\n",
        "    def __init__(self, palavras_chave=None):\n",
        "        self.vagas_encontradas = []\n",
        "        self.vagas_unicas = {}  # Dicion√°rio para controlar vagas √∫nicas por hash\n",
        "        self.total_vagas = 0\n",
        "        self.palavras_chave = palavras_chave or PALAVRAS_CHAVE\n",
        "        self.setup_selenium()\n",
        "\n",
        "    def setup_selenium(self):\n",
        "        \"\"\"Configura o driver do Selenium\"\"\"\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
        "\n",
        "        try:\n",
        "            self.driver = webdriver.Chrome(\n",
        "                ChromeDriverManager().install(),\n",
        "                options=chrome_options\n",
        "            )\n",
        "            self.driver.set_page_load_timeout(30)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao configurar Selenium: {e}\")\n",
        "            self.driver = None\n",
        "\n",
        "    def gerar_hash_vaga(self, titulo, empresa, localizacao):\n",
        "        \"\"\"Gera um hash √∫nico para identificar vagas duplicadas\"\"\"\n",
        "        texto = f\"{titulo}_{empresa}_{localizacao}\".lower().replace(\" \", \"\")\n",
        "        return hashlib.md5(texto.encode()).hexdigest()\n",
        "\n",
        "    def adicionar_vaga_se_unicas(self, vaga):\n",
        "        \"\"\"Adiciona uma vaga apenas se for √∫nica\"\"\"\n",
        "        vaga_hash = self.gerar_hash_vaga(vaga['titulo'], vaga['empresa'], vaga['localizacao'])\n",
        "\n",
        "        if vaga_hash not in self.vagas_unicas:\n",
        "            self.vagas_unicas[vaga_hash] = True\n",
        "            self.vagas_encontradas.append(vaga)\n",
        "            self.total_vagas += 1\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def buscar_vagas_linkedin(self):\n",
        "        \"\"\"Busca vagas no LinkedIn usando scraping\"\"\"\n",
        "        if not self.driver:\n",
        "            print(\"Driver do Selenium n√£o dispon√≠vel para LinkedIn\")\n",
        "            return\n",
        "\n",
        "        print(\"Buscando vagas no LinkedIn...\")\n",
        "\n",
        "        try:\n",
        "            # Buscar por termos mais gen√©ricos primeiro\n",
        "            termos_busca = [\"dados\", \"data\", \"analytics\", \"business intelligence\"]\n",
        "\n",
        "            for termo in termos_busca:\n",
        "                try:\n",
        "                    # Construir URL de busca com termos gen√©ricos\n",
        "                    keywords_encoded = termo.replace(\" \", \"%20\")\n",
        "                    location_encoded = LOCALIZACAO.replace(\" \", \"%20\").replace(\",\", \"%2C\")\n",
        "\n",
        "                    url = f\"https://www.linkedin.com/jobs/search/?keywords={keywords_encoded}&location={location_encoded}&f_TPR=r2592000\"\n",
        "\n",
        "                    self.driver.get(url)\n",
        "                    time.sleep(5)  # Esperar carregar\n",
        "\n",
        "                    # Scroll para carregar mais vagas\n",
        "                    for _ in range(5):\n",
        "                        self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "                        time.sleep(2)\n",
        "\n",
        "                    # Coletar elementos de vagas\n",
        "                    vagas = self.driver.find_elements(By.CSS_SELECTOR, '.jobs-search__results-list li')\n",
        "\n",
        "                    for i, vaga in enumerate(vagas):\n",
        "                        try:\n",
        "                            titulo = vaga.find_element(By.CSS_SELECTOR, '.base-search-card__title').text\n",
        "                            empresa = vaga.find_element(By.CSS_SELECTOR, '.base-search-card__subtitle').text\n",
        "                            localizacao = vaga.find_element(By.CSS_SELECTOR, '.job-search-card__location').text\n",
        "                            link = vaga.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')\n",
        "\n",
        "                            # Verificar se a vaga corresponde √†s nossas palavras-chave\n",
        "                            titulo_lower = titulo.lower()\n",
        "                            corresponde = any(palavra.lower() in titulo_lower for palavra in self.palavras_chave)\n",
        "\n",
        "                            if corresponde:\n",
        "                                # A data n√£o √© facilmente acess√≠vel, usar data atual como aproxima√ß√£o\n",
        "                                data_publicacao = datetime.now()\n",
        "\n",
        "                                if data_publicacao >= DATA_INICIO:\n",
        "                                    nova_vaga = {\n",
        "                                        'titulo': titulo,\n",
        "                                        'empresa': empresa,\n",
        "                                        'localizacao': localizacao,\n",
        "                                        'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                                        'fonte': 'LinkedIn',\n",
        "                                        'link': link,\n",
        "                                        'descricao': 'Verifique no site para detalhes completos',\n",
        "                                        'palavras_chave': ', '.join([p for p in self.palavras_chave if p.lower() in titulo_lower])\n",
        "                                    }\n",
        "\n",
        "                                    # Adicionar apenas se for √∫nica\n",
        "                                    self.adicionar_vaga_se_unicas(nova_vaga)\n",
        "                        except Exception as e:\n",
        "                            continue\n",
        "\n",
        "                    print(f\"LinkedIn - '{termo}' processado. Total: {self.total_vagas} vagas\")\n",
        "                    time.sleep(3)  # Respeitar o site\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao buscar '{termo}' no LinkedIn: {e}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro geral ao acessar LinkedIn: {e}\")\n",
        "\n",
        "    def buscar_vagas_indeed(self):\n",
        "        \"\"\"Busca vagas no Indeed Brasil\"\"\"\n",
        "        print(\"Buscando vagas no Indeed Brasil...\")\n",
        "\n",
        "        try:\n",
        "            base_url = \"https://br.indeed.com/jobs\"\n",
        "\n",
        "            # Usar termos gen√©ricos para evitar duplicatas\n",
        "            termos_busca = [\"dados\", \"analista\", \"cientista\", \"business intelligence\"]\n",
        "\n",
        "            for termo in termos_busca:\n",
        "                try:\n",
        "                    params = {\n",
        "                        'q': termo,\n",
        "                        'l': LOCALIZACAO,\n",
        "                        'fromage': '7',  # √öltimos 7 dias\n",
        "                        'start': 0\n",
        "                    }\n",
        "\n",
        "                    for pagina in range(0, 3):  # 3 p√°ginas por termo\n",
        "                        params['start'] = pagina * 10\n",
        "                        headers = {\n",
        "                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "                        }\n",
        "\n",
        "                        response = requests.get(base_url, params=params, headers=headers, timeout=15)\n",
        "\n",
        "                        if response.status_code != 200:\n",
        "                            print(f\"Erro no Indeed: {response.status_code}\")\n",
        "                            break\n",
        "\n",
        "                        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                        resultados = soup.find_all('div', class_='job_seen_beacon')\n",
        "\n",
        "                        if not resultados:\n",
        "                            break\n",
        "\n",
        "                        for vaga in resultados:\n",
        "                            try:\n",
        "                                titulo_elem = vaga.find('h2', class_='jobTitle')\n",
        "                                titulo = titulo_elem.text.strip() if titulo_elem else 'N/A'\n",
        "\n",
        "                                # Verificar se a vaga corresponde √†s nossas palavras-chave\n",
        "                                titulo_lower = titulo.lower()\n",
        "                                corresponde = any(palavra.lower() in titulo_lower for palavra in self.palavras_chave)\n",
        "\n",
        "                                if not corresponde:\n",
        "                                    continue\n",
        "\n",
        "                                empresa_elem = vaga.find('span', class_='companyName')\n",
        "                                empresa = empresa_elem.text.strip() if empresa_elem else 'N/A'\n",
        "\n",
        "                                local_elem = vaga.find('div', class_='companyLocation')\n",
        "                                localizacao = local_elem.text.strip() if local_elem else LOCALIZACAO\n",
        "\n",
        "                                data_elem = vaga.find('span', class_='date')\n",
        "                                data_texto = data_elem.text.strip() if data_elem else ''\n",
        "\n",
        "                                # Converter texto de data para objeto datetime\n",
        "                                data_publicacao = self.parse_data_indeed(data_texto)\n",
        "\n",
        "                                if data_publicacao >= DATA_INICIO:\n",
        "                                    link_elem = vaga.find('a', class_='jcs-JobTitle')\n",
        "                                    link = \"https://br.indeed.com\" + link_elem['href'] if link_elem else '#'\n",
        "\n",
        "                                    nova_vaga = {\n",
        "                                        'titulo': titulo,\n",
        "                                        'empresa': empresa,\n",
        "                                        'localizacao': localizacao,\n",
        "                                        'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                                        'fonte': 'Indeed Brasil',\n",
        "                                        'link': link,\n",
        "                                        'descricao': 'Verifique no site para detalhes completos',\n",
        "                                        'palavras_chave': ', '.join([p for p in self.palavras_chave if p.lower() in titulo_lower])\n",
        "                                    }\n",
        "\n",
        "                                    # Adicionar apenas se for √∫nica\n",
        "                                    self.adicionar_vaga_se_unicas(nova_vaga)\n",
        "                            except Exception as e:\n",
        "                                continue\n",
        "\n",
        "                        print(f\"Indeed - '{termo}' - P√°gina {pagina + 1} processada. Total: {self.total_vagas} vagas\")\n",
        "                        time.sleep(2)  # Respeitar o site\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao buscar '{termo}' no Indeed: {e}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro geral ao acessar Indeed: {e}\")\n",
        "\n",
        "    def parse_data_indeed(self, data_texto):\n",
        "        \"\"\"Converte o texto de data do Indeed para objeto datetime\"\"\"\n",
        "        try:\n",
        "            data_texto = data_texto.lower()\n",
        "            hoje = datetime.now()\n",
        "\n",
        "            if 'hoje' in data_texto or 'just' in data_texto:\n",
        "                return hoje\n",
        "            elif 'ontem' in data_texto:\n",
        "                return hoje - timedelta(days=1)\n",
        "            elif 'dias' in data_texto:\n",
        "                num_dias = int(re.search(r'(\\d+)', data_texto).group(1))\n",
        "                return hoje - timedelta(days=num_dias)\n",
        "            else:\n",
        "                # Tentar parse de data completa\n",
        "                try:\n",
        "                    return datetime.strptime(data_texto, '%d/%m/%Y')\n",
        "                except:\n",
        "                    return hoje  # Fallback para data atual\n",
        "        except:\n",
        "            return datetime.now()  # Fallback para data atual\n",
        "\n",
        "    def buscar_vagas_infojobs(self):\n",
        "        \"\"\"Busca vagas no InfoJobs\"\"\"\n",
        "        print(\"Buscando vagas no InfoJobs...\")\n",
        "\n",
        "        try:\n",
        "            base_url = \"https://www.infojobs.com.br/vagas-de-emprego\"\n",
        "\n",
        "            # Usar termos gen√©ricos\n",
        "            termos_busca = [\"dados\", \"analista\", \"cientista\", \"bi\"]\n",
        "\n",
        "            for termo in termos_busca:\n",
        "                try:\n",
        "                    params = {\n",
        "                        'palavraChave': termo,\n",
        "                        'cidade': 'S√£o Paulo',\n",
        "                        'pagina': 1\n",
        "                    }\n",
        "\n",
        "                    for pagina in range(1, 4):  # 3 p√°ginas por termo\n",
        "                        params['pagina'] = pagina\n",
        "                        headers = {\n",
        "                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "                        }\n",
        "\n",
        "                        response = requests.get(base_url, params=params, headers=headers, timeout=15)\n",
        "\n",
        "                        if response.status_code != 200:\n",
        "                            print(f\"Erro no InfoJobs: {response.status_code}\")\n",
        "                            break\n",
        "\n",
        "                        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                        resultados = soup.select('div.vaga')\n",
        "\n",
        "                        if not resultados:\n",
        "                            break\n",
        "\n",
        "                        for vaga in resultados:\n",
        "                            try:\n",
        "                                titulo_elem = vaga.select_one('h2.vagaTitle a')\n",
        "                                titulo = titulo_elem.text.strip() if titulo_elem else 'N/A'\n",
        "\n",
        "                                # Verificar se a vaga corresponde √†s nossas palavras-chave\n",
        "                                titulo_lower = titulo.lower()\n",
        "                                corresponde = any(palavra.lower() in titulo_lower for palavra in self.palavras_chave)\n",
        "\n",
        "                                if not corresponde:\n",
        "                                    continue\n",
        "\n",
        "                                empresa_elem = vaga.select_one('span.vaga-company')\n",
        "                                empresa = empresa_elem.text.strip() if empresa_elem else 'N/A'\n",
        "\n",
        "                                local_elem = vaga.select_one('span.vaga-location')\n",
        "                                localizacao = local_elem.text.strip() if local_elem else LOCALIZACAO\n",
        "\n",
        "                                data_elem = vaga.select_one('span.vaga-date')\n",
        "                                data_texto = data_elem.text.strip() if data_elem else ''\n",
        "\n",
        "                                # Converter texto de data para objeto datetime\n",
        "                                data_publicacao = self.parse_data_infojobs(data_texto)\n",
        "\n",
        "                                if data_publicacao >= DATA_INICIO:\n",
        "                                    link = titulo_elem['href'] if titulo_elem else '#'\n",
        "                                    if not link.startswith('http'):\n",
        "                                        link = 'https://www.infojobs.com.br' + link\n",
        "\n",
        "                                    nova_vaga = {\n",
        "                                        'titulo': titulo,\n",
        "                                        'empresa': empresa,\n",
        "                                        'localizacao': localizacao,\n",
        "                                        'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                                        'fonte': 'InfoJobs',\n",
        "                                        'link': link,\n",
        "                                        'descricao': 'Verifique no site para detalhes completos',\n",
        "                                        'palavras_chave': ', '.join([p for p in self.palavras_chave if p.lower() in titulo_lower])\n",
        "                                    }\n",
        "\n",
        "                                    # Adicionar apenas se for √∫nica\n",
        "                                    self.adicionar_vaga_se_unicas(nova_vaga)\n",
        "                            except Exception as e:\n",
        "                                continue\n",
        "\n",
        "                        print(f\"InfoJobs - '{termo}' - P√°gina {pagina} processada. Total: {self.total_vagas} vagas\")\n",
        "                        time.sleep(2)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao buscar '{termo}' no InfoJobs: {e}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro geral ao acessar InfoJobs: {e}\")\n",
        "\n",
        "    def parse_data_infojobs(self, data_texto):\n",
        "        \"\"\"Converte o texto de data do InfoJobs para objeto datetime\"\"\"\n",
        "        try:\n",
        "            # Formato comum: \"Publicada em 15/08/2024\"\n",
        "            match = re.search(r'(\\d{2}/\\d{2}/\\d{4})', data_texto)\n",
        "            if match:\n",
        "                return datetime.strptime(match.group(1), '%d/%m/%Y')\n",
        "\n",
        "            # Outros formatos com dias relativos\n",
        "            data_texto = data_texto.lower()\n",
        "            hoje = datetime.now()\n",
        "\n",
        "            if 'hoje' in data_texto:\n",
        "                return hoje\n",
        "            elif 'ontem' in data_texto:\n",
        "                return hoje - timedelta(days=1)\n",
        "            elif 'dias' in data_texto:\n",
        "                num_dias = int(re.search(r'(\\d+)', data_texto).group(1))\n",
        "                return hoje - timedelta(days=num_dias)\n",
        "            else:\n",
        "                return hoje  # Fallback\n",
        "        except:\n",
        "            return datetime.now()  # Fallback\n",
        "\n",
        "    def buscar_vagas_brasil(self):\n",
        "        \"\"\"Busca em sites de empregos brasileiros alternativos\"\"\"\n",
        "        print(\"Buscando em sites brasileiros alternativos...\")\n",
        "\n",
        "        # Lista de sites para buscar\n",
        "        sites = [\n",
        "            {\n",
        "                'nome': 'Vagas.com',\n",
        "                'url': \"https://www.vagas.com.br/vagas-de-tecnologia-em-sao-paulo\",\n",
        "                'selector': '.vaga',\n",
        "                'titulo_selector': '.cargo',\n",
        "                'empresa_selector': '.empr',\n",
        "                'local_selector': '.vaga-local',\n",
        "                'data_selector': '.data'\n",
        "            },\n",
        "            {\n",
        "                'nome': 'Catho',\n",
        "                'url': \"https://www.catho.com.br/vagas/tecnologia/em-sao-paulo-sp/\",\n",
        "                'selector': '.job-card',\n",
        "                'titulo_selector': '.job-card__title',\n",
        "                'empresa_selector': '.job-card__company',\n",
        "                'local_selector': '.job-card__location',\n",
        "                'data_selector': '.job-card__date'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        for site in sites:\n",
        "            try:\n",
        "                headers = {\n",
        "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "                }\n",
        "\n",
        "                response = requests.get(site['url'], headers=headers, timeout=15)\n",
        "                if response.status_code != 200:\n",
        "                    continue\n",
        "\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                vagas = soup.select(site['selector'])\n",
        "\n",
        "                for vaga in vagas[:20]:  # Limitar a 20 vagas por site\n",
        "                    try:\n",
        "                        titulo = vaga.select_one(site['titulo_selector']).text.strip() if vaga.select_one(site['titulo_selector']) else 'N/A'\n",
        "\n",
        "                        # Verificar se a vaga corresponde √†s nossas palavras-chave\n",
        "                        titulo_lower = titulo.lower()\n",
        "                        corresponde = any(palavra.lower() in titulo_lower for palavra in self.palavras_chave)\n",
        "\n",
        "                        if not corresponde:\n",
        "                            continue\n",
        "\n",
        "                        empresa = vaga.select_one(site['empresa_selector']).text.strip() if vaga.select_one(site['empresa_selector']) else 'N/A'\n",
        "                        localizacao = vaga.select_one(site['local_selector']).text.strip() if vaga.select_one(site['local_selector']) else LOCALIZACAO\n",
        "\n",
        "                        # Usar data atual como fallback\n",
        "                        data_publicacao = datetime.now()\n",
        "\n",
        "                        nova_vaga = {\n",
        "                            'titulo': titulo,\n",
        "                            'empresa': empresa,\n",
        "                            'localizacao': localizacao,\n",
        "                            'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                            'fonte': site['nome'],\n",
        "                            'link': site['url'],\n",
        "                            'descricao': 'Verifique no site para detalhes completos',\n",
        "                            'palavras_chave': ', '.join([p for p in self.palavras_chave if p.lower() in titulo_lower])\n",
        "                        }\n",
        "\n",
        "                        # Adicionar apenas se for √∫nica\n",
        "                        self.adicionar_vaga_se_unicas(nova_vaga)\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                print(f\"{site['nome']} processado. Total: {self.total_vagas} vagas\")\n",
        "                time.sleep(2)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao acessar {site['nome']}: {e}\")\n",
        "                continue\n",
        "\n",
        "    def buscar_todas_vagas(self):\n",
        "        \"\"\"Executa todas as buscas\"\"\"\n",
        "        print(\"Iniciando busca por vagas em S√£o Paulo...\")\n",
        "        print(f\"Per√≠odo: desde {DATA_INICIO.strftime('%d/%m/%Y')} at√© hoje\")\n",
        "        print(f\"Localiza√ß√£o: {LOCALIZACAO}\")\n",
        "        print(f\"Palavras-chave: {', '.join(self.palavras_chave)}\")\n",
        "\n",
        "        # Executar todas as buscas\n",
        "        self.buscar_vagas_linkedin()\n",
        "        self.buscar_vagas_indeed()\n",
        "        self.buscar_vagas_infojobs()\n",
        "        self.buscar_vagas_brasil()\n",
        "\n",
        "        print(f\"Busca conclu√≠da! Total de {self.total_vagas} vagas √∫nicas encontradas.\")\n",
        "\n",
        "    def exportar_resultados(self):\n",
        "        \"\"\"Exporta os resultados para Excel\"\"\"\n",
        "        if not self.vagas_encontradas:\n",
        "            print(\"Nenhuma vaga encontrada para exportar\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(self.vagas_encontradas)\n",
        "\n",
        "        # Reordenar colunas\n",
        "        colunas = ['titulo', 'empresa', 'localizacao', 'data_publicacao', 'fonte', 'palavras_chave', 'descricao', 'link']\n",
        "        df = df[colunas]\n",
        "\n",
        "        df.to_excel(OUTPUT_PATH, index=False)\n",
        "        print(f\"Resultados exportados para: {OUTPUT_PATH}\")\n",
        "\n",
        "    def mostrar_resultados(self):\n",
        "        \"\"\"Mostra os resultados na tela\"\"\"\n",
        "        if not self.vagas_encontradas:\n",
        "            print(\"Nenhuma vaga encontrada\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nVAGAS ENCONTRADAS: {self.total_vagas}\")\n",
        "        print(\"=\" * 200)\n",
        "\n",
        "        for i, vaga in enumerate(self.vagas_encontradas[:2000], 1):  # Mostrar apenas as 20 primeiras\n",
        "            print(f\"\\n{i}. {vaga['titulo']}\")\n",
        "            print(f\"   {vaga['empresa']}\")\n",
        "            print(f\"   {vaga['localizacao']}\")\n",
        "            print(f\"   {vaga['data_publicacao']}\")\n",
        "            print(f\"   {vaga['fonte']}\")\n",
        "            print(f\"   {vaga['palavras_chave']}\")\n",
        "            print(f\"   {vaga['link']}\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "        if self.total_vagas > 20:\n",
        "            print(f\"\\n... e mais {self.total_vagas - 2000} vagas (ver arquivo Excel para lista completa)\")\n",
        "\n",
        "# Executar a busca\n",
        "if __name__ == \"__main__\":\n",
        "    finder = VagasInteligenteFinder(palavras_chave=PALAVRAS_CHAVE)\n",
        "    finder.buscar_todas_vagas()\n",
        "    finder.mostrar_resultados()\n",
        "    finder.exportar_resultados()\n",
        "\n",
        "    # Fechar o driver do Selenium se existir\n",
        "    if hasattr(finder, 'driver') and finder.driver:\n",
        "        finder.driver.quit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwO5D3LLEWog",
        "outputId": "282d9489-a9d1-457b-b1b5-baad6b97eac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configurando ambiente...\n",
            "Ambiente configurado!\n",
            "Importando bibliotecas...\n",
            "Erro ao configurar Selenium: WebDriver.__init__() got multiple values for argument 'options'\n",
            "Iniciando busca por vagas em S√£o Paulo...\n",
            "Per√≠odo: desde 01/08/2024 at√© hoje\n",
            "Localiza√ß√£o: S√£o Paulo, SP\n",
            "Palavras-chave: analista de dados, data analyst, cientista de dados, data scientist, sql, python, business intelligence, preditivo, predictive, modelagem, etl, Manipula√ß√£o de Dados, Excel, Big Data, Machine Learning, estat√≠stica, power bi, tableau, data warehouse\n",
            "Driver do Selenium n√£o dispon√≠vel para LinkedIn\n",
            "Buscando vagas no Indeed Brasil...\n",
            "Erro no Indeed: 403\n",
            "Erro no Indeed: 403\n",
            "Erro no Indeed: 403\n",
            "Erro no Indeed: 403\n",
            "Buscando vagas no InfoJobs...\n",
            "Buscando em sites brasileiros alternativos...\n",
            "Vagas.com processado. Total: 0 vagas\n",
            "Busca conclu√≠da! Total de 0 vagas √∫nicas encontradas.\n",
            "Nenhuma vaga encontrada\n",
            "Nenhuma vaga encontrada para exportar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Sistema de Busca de Vagas com APIs Oficiais\n",
        "Autor: Assistente AI\n",
        "Data: 2025\n",
        "\"\"\"\n",
        "\n",
        "print(\"üîß Configurando ambiente...\")\n",
        "\n",
        "# Instalar bibliotecas necess√°rias\n",
        "!pip install requests pandas datetime python-dotenv --quiet\n",
        "\n",
        "print(\"Ambiente configurado!\")\n",
        "print(\"Importando bibliotecas...\")\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import json\n",
        "\n",
        "# Carregar vari√°veis de ambiente\n",
        "load_dotenv()\n",
        "\n",
        "# Configura√ß√µes\n",
        "OUTPUT_PATH = \"/content/sample_data/vagas_apis_oficiais.xlsx\"\n",
        "DATA_INICIO = datetime(2024, 8, 1)  # 01/08/2024\n",
        "LOCALIZACAO = \"S√£o Paulo, SP\"\n",
        "\n",
        "# Palavras-chave priorit√°rias para √°rea de dados\n",
        "PALAVRAS_CHAVE = [\n",
        "    \"analista de dados\", \"data analyst\", \"cientista de dados\", \"data scientist\",\n",
        "    \"sql\", \"python\", \"business intelligence\", \"preditivo\", \"predictive\",\n",
        "    \"modelagem\", \"etl\", \"Manipula√ß√£o de Dados\", \"Excel\", \"Big Data\",\n",
        "    \"Machine Learning\", \"estat√≠stica\", \"power bi\", \"tableau\", \"data warehouse\"\n",
        "]\n",
        "\n",
        "class APIVagasFinder:\n",
        "    def __init__(self, palavras_chave=None):\n",
        "        self.vagas_encontradas = []\n",
        "        self.total_vagas = 0\n",
        "        self.palavras_chave = palavras_chave or PALAVRAS_CHAVE\n",
        "\n",
        "        # Configurar credenciais de APIs\n",
        "        self.infojobs_client_id = os.getenv('INFOJOBS_CLIENT_ID', 'seu_client_id_aqui')\n",
        "        self.infojobs_client_secret = os.getenv('INFOJOBS_CLIENT_SECRET', 'seu_client_secret_aqui')\n",
        "        self.serpapi_key = os.getenv('SERPAPI_KEY', 'sua_chave_serpapi_aqui')\n",
        "\n",
        "    def buscar_vagas_infojobs_api(self):\n",
        "        \"\"\"Busca vagas usando a API oficial do InfoJobs :cite[1]:cite[5]:cite[8]\"\"\"\n",
        "        print(\"Buscando vagas via API do InfoJobs...\")\n",
        "\n",
        "        try:\n",
        "            # Autentica√ß√£o na API do InfoJobs\n",
        "            auth_url = \"https://www.infojobs.com.br/oauth/authorize\"\n",
        "            token_url = \"https://www.infojobs.com.br/oauth/token\"\n",
        "\n",
        "            # Obter token de acesso (simplificado - na pr√°tica precisa do fluxo OAuth completo)\n",
        "            # Para uso real, implemente o fluxo OAuth2 conforme documenta√ß√£o\n",
        "            auth_params = {\n",
        "                'grant_type': 'client_credentials',\n",
        "                'client_id': self.infojobs_client_id,\n",
        "                'client_secret': self.infojobs_client_secret,\n",
        "                'scope': 'api_candidatura'\n",
        "            }\n",
        "\n",
        "            # Para demonstra√ß√£o, vamos usar o endpoint de busca p√∫blica\n",
        "            for palavra_chave in self.palavras_chave[:5]:  # Limitar a 5 palavras-chave\n",
        "                try:\n",
        "                    url = \"https://api.infojobs.net/api/7/offer\"\n",
        "                    params = {\n",
        "                        'q': palavra_chave,\n",
        "                        'city': 'S√£o Paulo',\n",
        "                        'sortRelevance': 'true',\n",
        "                        'maxResults': 50\n",
        "                    }\n",
        "\n",
        "                    headers = {\n",
        "                        'Authorization': f'Basic {self.infojobs_client_id}:{self.infojobs_client_secret}',\n",
        "                        'Content-Type': 'application/json',\n",
        "                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "                    }\n",
        "\n",
        "                    response = requests.get(url, params=params, headers=headers, timeout=15)\n",
        "\n",
        "                    if response.status_code == 200:\n",
        "                        dados = response.json()\n",
        "\n",
        "                        if 'items' in dados:\n",
        "                            for vaga in dados['items']:\n",
        "                                try:\n",
        "                                    # Converter data para objeto datetime\n",
        "                                    data_texto = vaga.get('creationDate', '')\n",
        "                                    data_publicacao = datetime.strptime(data_texto, '%Y-%m-%dT%H:%M:%S.%fZ') if data_texto else datetime.now()\n",
        "\n",
        "                                    if data_publicacao >= DATA_INICIO:\n",
        "                                        self.vagas_encontradas.append({\n",
        "                                            'titulo': vaga.get('title', 'N/A'),\n",
        "                                            'empresa': vaga.get('author', {}).get('name', 'N/A'),\n",
        "                                            'localizacao': vaga.get('city', LOCALIZACAO),\n",
        "                                            'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                                            'fonte': 'InfoJobs API',\n",
        "                                            'link': vaga.get('link', '#'),\n",
        "                                            'descricao': vaga.get('description', '')[:200] + '...' if vaga.get('description') else 'N/A',\n",
        "                                            'palavra_chave': palavra_chave\n",
        "                                        })\n",
        "                                        self.total_vagas += 1\n",
        "                                except Exception as e:\n",
        "                                    continue\n",
        "\n",
        "                            print(f\"InfoJobs API - '{palavra_chave}' processada. Total: {self.total_vagas} vagas\")\n",
        "                        else:\n",
        "                            print(f\"Nenhum resultado encontrado no InfoJobs API para '{palavra_chave}'\")\n",
        "\n",
        "                    time.sleep(1)  # Respeitar a API\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao buscar '{palavra_chave}' no InfoJobs API: {e}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro geral ao acessar InfoJobs API: {e}\")\n",
        "\n",
        "    def buscar_vagas_google_jobs_api(self):\n",
        "        \"\"\"Busca vagas usando Google Jobs API atrav√©s do SerpApi\"\"\"\n",
        "        print(\"Buscando vagas via Google Jobs API...\")\n",
        "\n",
        "        try:\n",
        "            if self.serpapi_key == 'sua_chave_serpapi_aqui':\n",
        "                print(\"Chave SerpAPI n√£o configurada. Pulando Google Jobs API.\")\n",
        "                return\n",
        "\n",
        "            # Para cada palavra-chave, fazer uma busca\n",
        "            for palavra_chave in self.palavras_chave[:3]:  # Limitar a 3 palavras-chave\n",
        "                try:\n",
        "                    params = {\n",
        "                        \"engine\": \"google_jobs\",\n",
        "                        \"q\": f\"{palavra_chave} {LOCALIZACAO}\",\n",
        "                        \"hl\": \"pt\",\n",
        "                        \"gl\": \"br\",\n",
        "                        \"api_key\": self.serpapi_key\n",
        "                    }\n",
        "\n",
        "                    search_url = \"https://serpapi.com/search\"\n",
        "                    response = requests.get(search_url, params=params, timeout=15)\n",
        "\n",
        "                    if response.status_code == 200:\n",
        "                        results = response.json()\n",
        "\n",
        "                        if \"jobs_results\" in results:\n",
        "                            for vaga in results[\"jobs_results\"]:\n",
        "                                try:\n",
        "                                    # Converter data para objeto datetime\n",
        "                                    data_texto = vaga.get(\"detected_extensions\", {}).get(\"posted_at\", \"\")\n",
        "                                    data_publicacao = self.parse_data_google_jobs(data_texto)\n",
        "\n",
        "                                    if data_publicacao >= DATA_INICIO:\n",
        "                                        self.vagas_encontradas.append({\n",
        "                                            'titulo': vaga.get('title', 'N/A'),\n",
        "                                            'empresa': vaga.get('company_name', 'N/A'),\n",
        "                                            'localizacao': vaga.get('location', LOCALIZACAO),\n",
        "                                            'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                                            'fonte': 'Google Jobs API',\n",
        "                                            'link': vaga.get('related_links', [{}])[0].get('link', '#') if vaga.get('related_links') else '#',\n",
        "                                            'descricao': vaga.get('description', '')[:200] + '...' if vaga.get('description') else 'N/A',\n",
        "                                            'palavra_chave': palavra_chave\n",
        "                                        })\n",
        "                                        self.total_vagas += 1\n",
        "                                except Exception as e:\n",
        "                                    continue\n",
        "\n",
        "                            print(f\"Google Jobs API - '{palavra_chave}' processada. Total: {self.total_vagas} vagas\")\n",
        "                        else:\n",
        "                            print(f\"Nenhum resultado encontrado no Google Jobs API para '{palavra_chave}'\")\n",
        "\n",
        "                    time.sleep(2)  # Respeitar a API\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao buscar '{palavra_chave}' no Google Jobs API: {e}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro geral ao acessar Google Jobs API: {e}\")\n",
        "\n",
        "    def parse_data_google_jobs(self, data_texto):\n",
        "        \"\"\"Converte o texto de data do Google Jobs para objeto datetime\"\"\"\n",
        "        try:\n",
        "            data_texto = data_texto.lower()\n",
        "            hoje = datetime.now()\n",
        "\n",
        "            if 'hour' in data_texto or 'hora' in data_texto:\n",
        "                return hoje\n",
        "            elif 'day' in data_texto or 'dia' in data_texto:\n",
        "                num_dias = int(re.search(r'(\\d+)', data_texto).group(1))\n",
        "                return hoje - timedelta(days=num_dias)\n",
        "            elif 'week' in data_texto or 'semana' in data_texto:\n",
        "                num_semanas = int(re.search(r'(\\d+)', data_texto).group(1))\n",
        "                return hoje - timedelta(weeks=num_semanas)\n",
        "            else:\n",
        "                return hoje  # Fallback para data atual\n",
        "        except:\n",
        "            return datetime.now()  # Fallback para data atual\n",
        "\n",
        "    def buscar_vagas_adzuna_api(self):\n",
        "        \"\"\"Busca vagas usando a API do Adzuna\"\"\"\n",
        "        print(\"Buscando vagas via Adzuna API...\")\n",
        "\n",
        "        try:\n",
        "            # Configura√ß√£o da API Adzuna (requer registro)\n",
        "            app_id = os.getenv('ADZUNA_APP_ID', 'seu_app_id_aqui')\n",
        "            app_key = os.getenv('ADZUNA_APP_KEY', 'seu_app_key_aqui')\n",
        "            pais = \"br\"  # Brasil\n",
        "\n",
        "            if app_id == 'seu_app_id_aqui' or app_key == 'seu_app_key_aqui':\n",
        "                print(\"Credenciais Adzuna n√£o configuradas. Pulando Adzuna API.\")\n",
        "                return\n",
        "\n",
        "            for palavra_chave in self.palavras_chave[:3]:  # Limitar a 3 palavras-chave\n",
        "                try:\n",
        "                    url = f\"https://api.adzuna.com/v1/api/jobs/{pais}/search/1\"\n",
        "                    params = {\n",
        "                        'app_id': app_id,\n",
        "                        'app_key': app_key,\n",
        "                        'results_per_page': 20,\n",
        "                        'what': palavra_chave,\n",
        "                        'where': 'S√£o Paulo',\n",
        "                        'content-type': 'application/json'\n",
        "                    }\n",
        "\n",
        "                    response = requests.get(url, params=params, timeout=15)\n",
        "\n",
        "                    if response.status_code == 200:\n",
        "                        dados = response.json()\n",
        "\n",
        "                        if 'results' in dados:\n",
        "                            for vaga in dados['results']:\n",
        "                                try:\n",
        "                                    # Converter data para objeto datetime\n",
        "                                    data_texto = vaga.get('created', '')\n",
        "                                    data_publicacao = datetime.strptime(data_texto, '%Y-%m-%dT%H:%M:%SZ') if data_texto else datetime.now()\n",
        "\n",
        "                                    if data_publicacao >= DATA_INICIO:\n",
        "                                        self.vagas_encontradas.append({\n",
        "                                            'titulo': vaga.get('title', 'N/A'),\n",
        "                                            'empresa': vaga.get('company', {}).get('display_name', 'N/A'),\n",
        "                                            'localizacao': vaga.get('location', {}).get('display_name', LOCALIZACAO),\n",
        "                                            'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                                            'fonte': 'Adzuna API',\n",
        "                                            'link': vaga.get('redirect_url', '#'),\n",
        "                                            'descricao': vaga.get('description', '')[:200] + '...' if vaga.get('description') else 'N/A',\n",
        "                                            'palavra_chave': palavra_chave\n",
        "                                        })\n",
        "                                        self.total_vagas += 1\n",
        "                                except Exception as e:\n",
        "                                    continue\n",
        "\n",
        "                            print(f\"Adzuna API - '{palavra_chave}' processada. Total: {self.total_vagas} vagas\")\n",
        "                        else:\n",
        "                            print(f\"Nenhum resultado encontrado no Adzuna API para '{palavra_chave}'\")\n",
        "\n",
        "                    time.sleep(1)  # Respeitar a API\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao buscar '{palavra_chave}' no Adzuna API: {e}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro geral ao acessar Adzuna API: {e}\")\n",
        "\n",
        "    def buscar_vagas_github_jobs_api(self):\n",
        "        \"\"\"Busca vagas usando a API do GitHub Jobs\"\"\"\n",
        "        print(\"Buscando vagas via GitHub Jobs API...\")\n",
        "\n",
        "        try:\n",
        "            for palavra_chave in self.palavras_chave[:3]:  # Limitar a 3 palavras-chave\n",
        "                try:\n",
        "                    url = \"https://jobs.github.com/positions.json\"\n",
        "                    params = {\n",
        "                        'description': palavra_chave,\n",
        "                        'location': 'S√£o Paulo'\n",
        "                    }\n",
        "\n",
        "                    response = requests.get(url, params=params, timeout=15)\n",
        "\n",
        "                    if response.status_code == 200:\n",
        "                        vagas = response.json()\n",
        "\n",
        "                        for vaga in vagas:\n",
        "                            try:\n",
        "                                # Converter data para objeto datetime\n",
        "                                data_texto = vaga.get('created_at', '')\n",
        "                                data_publicacao = datetime.strptime(data_texto, '%a %b %d %H:%M:%S UTC %Y') if data_texto else datetime.now()\n",
        "\n",
        "                                if data_publicacao >= DATA_INICIO:\n",
        "                                    self.vagas_encontradas.append({\n",
        "                                        'titulo': vaga.get('title', 'N/A'),\n",
        "                                        'empresa': vaga.get('company', 'N/A'),\n",
        "                                        'localizacao': vaga.get('location', LOCALIZACAO),\n",
        "                                        'data_publicacao': data_publicacao.strftime('%d/%m/%Y'),\n",
        "                                        'fonte': 'GitHub Jobs API',\n",
        "                                        'link': vaga.get('url', '#'),\n",
        "                                        'descricao': vaga.get('description', '')[:200] + '...' if vaga.get('description') else 'N/A',\n",
        "                                        'palavra_chave': palavra_chave\n",
        "                                    })\n",
        "                                    self.total_vagas += 1\n",
        "                            except Exception as e:\n",
        "                                continue\n",
        "\n",
        "                        print(f\"GitHub Jobs API - '{palavra_chave}' processada. Total: {self.total_vagas} vagas\")\n",
        "                    else:\n",
        "                        print(f\"Nenhum resultado encontrado no GitHub Jobs API para '{palavra_chave}'\")\n",
        "\n",
        "                    time.sleep(1)  # Respeitar a API\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao buscar '{palavra_chave}' no GitHub Jobs API: {e}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro geral ao acessar GitHub Jobs API: {e}\")\n",
        "\n",
        "    def remover_duplicatas(self):\n",
        "        \"\"\"Remove vagas duplicadas com base no t√≠tulo e empresa\"\"\"\n",
        "        if not self.vagas_encontradas:\n",
        "            return\n",
        "\n",
        "        # Usar pandas para remover duplicatas\n",
        "        df = pd.DataFrame(self.vagas_encontradas)\n",
        "        df = df.drop_duplicates(subset=['titulo', 'empresa'], keep='first')\n",
        "        self.vagas_encontradas = df.to_dict('records')\n",
        "        self.total_vagas = len(self.vagas_encontradas)\n",
        "\n",
        "    def buscar_todas_vagas(self):\n",
        "        \"\"\"Executa todas as buscas\"\"\"\n",
        "        print(\"Iniciando busca por vagas em S√£o Paulo usando APIs...\")\n",
        "        print(f\"Per√≠odo: desde {DATA_INICIO.strftime('%d/%m/%Y')} at√© hoje\")\n",
        "        print(f\"Localiza√ß√£o: {LOCALIZACAO}\")\n",
        "        print(f\"Palavras-chave: {', '.join(self.palavras_chave)}\")\n",
        "\n",
        "        # Executar todas as buscas por APIs\n",
        "        self.buscar_vagas_infojobs_api()\n",
        "        self.buscar_vagas_google_jobs_api()\n",
        "        self.buscar_vagas_adzuna_api()\n",
        "        self.buscar_vagas_github_jobs_api()\n",
        "\n",
        "        # Remover duplicatas\n",
        "        self.remover_duplicatas()\n",
        "\n",
        "        print(f\"Busca conclu√≠da! Total de {self.total_vagas} vagas √∫nicas encontradas.\")\n",
        "\n",
        "    def exportar_resultados(self):\n",
        "        \"\"\"Exporta os resultados para Excel\"\"\"\n",
        "        if not self.vagas_encontradas:\n",
        "            print(\"Nenhuma vaga encontrada para exportar\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(self.vagas_encontradas)\n",
        "\n",
        "        # Reordenar colunas\n",
        "        colunas = ['titulo', 'empresa', 'localizacao', 'data_publicacao', 'fonte', 'palavra_chave', 'descricao', 'link']\n",
        "        df = df[colunas]\n",
        "\n",
        "        df.to_excel(OUTPUT_PATH, index=False)\n",
        "        print(f\"Resultados exportados para: {OUTPUT_PATH}\")\n",
        "\n",
        "    def mostrar_resultados(self):\n",
        "        \"\"\"Mostra os resultados na tela\"\"\"\n",
        "        if not self.vagas_encontradas:\n",
        "            print(\"Nenhuma vaga encontrada\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nVAGAS ENCONTRADAS: {self.total_vagas}\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        for i, vaga in enumerate(self.vagas_encontradas[:15], 1):  # Mostrar apenas as 15 primeiras\n",
        "            print(f\"\\n{i}. {vaga['titulo']}\")\n",
        "            print(f\"   {vaga['empresa']}\")\n",
        "            print(f\"   {vaga['localizacao']}\")\n",
        "            print(f\"   {vaga['data_publicacao']}\")\n",
        "            print(f\"   {vaga['fonte']}\")\n",
        "            print(f\"   {vaga['palavra_chave']}\")\n",
        "            print(f\"   {vaga['descricao']}\")\n",
        "            print(f\"   {vaga['link']}\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "        if self.total_vagas > 15:\n",
        "            print(f\"\\n... e mais {self.total_vagas - 15} vagas (ver arquivo Excel para lista completa)\")\n",
        "\n",
        "# Executar a busca\n",
        "if __name__ == \"__main__\":\n",
        "    finder = APIVagasFinder(palavras_chave=PALAVRAS_CHAVE)\n",
        "    finder.buscar_todas_vagas()\n",
        "    finder.mostrar_resultados()\n",
        "    finder.exportar_resultados()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UDM6PHeWGkU",
        "outputId": "2ede536f-feed-4675-e2fe-7d45192c89c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Configurando ambiente...\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m264.4/264.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hAmbiente configurado!\n",
            "Importando bibliotecas...\n",
            "Iniciando busca por vagas em S√£o Paulo usando APIs...\n",
            "Per√≠odo: desde 01/08/2024 at√© hoje\n",
            "Localiza√ß√£o: S√£o Paulo, SP\n",
            "Palavras-chave: analista de dados, data analyst, cientista de dados, data scientist, sql, python, business intelligence, preditivo, predictive, modelagem, etl, Manipula√ß√£o de Dados, Excel, Big Data, Machine Learning, estat√≠stica, power bi, tableau, data warehouse\n",
            "Buscando vagas via API do InfoJobs...\n",
            "Buscando vagas via Google Jobs API...\n",
            " Chave SerpAPI n√£o configurada. Pulando Google Jobs API.\n",
            "Buscando vagas via Adzuna API...\n",
            " Credenciais Adzuna n√£o configuradas. Pulando Adzuna API.\n",
            "Buscando vagas via GitHub Jobs API...\n",
            "Erro ao buscar 'analista de dados' no GitHub Jobs API: HTTPSConnectionPool(host='jobs.github.com', port=443): Max retries exceeded with url: /positions.json?description=analista+de+dados&location=S%C3%A3o+Paulo (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7bee70b79370>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Erro ao buscar 'data analyst' no GitHub Jobs API: HTTPSConnectionPool(host='jobs.github.com', port=443): Max retries exceeded with url: /positions.json?description=data+analyst&location=S%C3%A3o+Paulo (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7bee4a1e33e0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Erro ao buscar 'cientista de dados' no GitHub Jobs API: HTTPSConnectionPool(host='jobs.github.com', port=443): Max retries exceeded with url: /positions.json?description=cientista+de+dados&location=S%C3%A3o+Paulo (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7bee50463b30>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Busca conclu√≠da! Total de 0 vagas √∫nicas encontradas.\n",
            "Nenhuma vaga encontrada\n",
            "Nenhuma vaga encontrada para exportar\n"
          ]
        }
      ]
    }
  ]
}
