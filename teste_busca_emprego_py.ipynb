{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmtaha/Projeto_busca_de_emprego/blob/main/teste_busca_emprego_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWBWepAXw0Jd",
        "outputId": "4602a682-8191-4f46-9370-8b6794899f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Etapa 1: Processamento do Currículo e Extração de Palavras-chave\n",
        "\n",
        "import os\n",
        "import docx\n",
        "import spacy\n",
        "import pytextrank\n",
        "import yake\n",
        "from rake_nltk import Rake\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "import re\n",
        "\n",
        "# Baixar recursos necessários\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Diretórios fixos\n",
        "DIR_CURR = \"/content/curriculo\"\n",
        "DIR_SAMPLE = \"/content/sample_data\"\n",
        "DIR_PROC = \"/content/Processamento\"\n",
        "DIR_OUT = \"/content/Output\"\n",
        "\n",
        "for path in [DIR_CURR, DIR_SAMPLE, DIR_PROC, DIR_OUT]:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# Credenciais\n",
        "CRED = {\n",
        "    #'linkedin': {'email': '********', 'senha': '******'},\n",
        "    #'glassdoor': {'email': '***********', 'senha': '**********'},\n",
        "    #'vagas': {'usuario': '*****', 'senha': '************'},\n",
        "    #'indeed': {'email': '***********', 'senha': '************'}\n",
        "}\n",
        "\n",
        "# Modelos spaCy e extratores\n",
        "nlpt = spacy.load(\"pt_core_news_sm\")\n",
        "nlpt.add_pipe(\"textrank\")\n",
        "\n",
        "# Extratores\n",
        "yake_extractor = yake.KeywordExtractor(lan=\"pt\", top=100, n=1)\n",
        "rake = Rake(language=\"portuguese\", stopwords=stopwords.words(\"portuguese\"))\n",
        "\n",
        "# POS tags a filtrar\n",
        "FILTER_POS = {'PRON', 'DET', 'ADP', 'AUX', 'CCONJ', 'SCONJ', 'PART', 'INTJ', 'PUNCT', 'SPACE', 'NUM'}\n",
        "\n",
        "# Utilitários\n",
        "\n",
        "def extrair_texto_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    return '\\n'.join([p.text for p in doc.paragraphs])\n",
        "\n",
        "def salvar_texto(texto, nome_arquivo):\n",
        "    with open(os.path.join(DIR_PROC, nome_arquivo), 'w', encoding='utf-8') as f:\n",
        "        f.write(texto)\n",
        "\n",
        "def limpar_com_filtragem(texto, modelo_spacy):\n",
        "    doc = modelo_spacy(texto)\n",
        "    tokens_filtrados = [token.lemma_.lower() for token in doc\n",
        "                        if token.pos_ not in FILTER_POS and not token.is_stop and token.is_alpha]\n",
        "    return ' '.join(tokens_filtrados)\n",
        "\n",
        "def extrair_keywords(texto, lang_model):\n",
        "    texto_limpo = limpar_com_filtragem(texto, lang_model)\n",
        "\n",
        "    # YAKE\n",
        "    keywords_yake = [kw for kw, score in yake_extractor.extract_keywords(texto_limpo)]\n",
        "\n",
        "    # RAKE\n",
        "    rake.extract_keywords_from_text(texto_limpo)\n",
        "    keywords_rake = rake.get_ranked_phrases()\n",
        "\n",
        "    # TextRank\n",
        "    doc = lang_model(texto_limpo)\n",
        "    keywords_textrank = [phrase.text.lower() for phrase in doc._.phrases]\n",
        "\n",
        "    return {\n",
        "        'yake': keywords_yake[:20],\n",
        "        'rake': keywords_rake[:20],\n",
        "        'textrank': keywords_textrank[:20]\n",
        "    }\n",
        "\n",
        "def processar_curriculos():\n",
        "    for arquivo in os.listdir(DIR_CURR):\n",
        "        if arquivo.endswith(\".docx\"):\n",
        "            caminho_arquivo = os.path.join(DIR_CURR, arquivo)\n",
        "            texto_extraido = extrair_texto_docx(caminho_arquivo)\n",
        "\n",
        "            # Salvar texto bruto\n",
        "            salvar_texto(texto_extraido, f\"{arquivo.replace('.docx', '')}_processado.txt\")\n",
        "\n",
        "            # Extrair palavras-chave\n",
        "            resultados = extrair_keywords(texto_extraido, nlpt)\n",
        "\n",
        "            # Salvar resultados\n",
        "            for metodo, palavras in resultados.items():\n",
        "                with open(os.path.join(DIR_OUT, f\"{arquivo.replace('.docx', '')}_{metodo}.txt\"), 'w', encoding='utf-8') as f:\n",
        "                    f.write('\\n'.join(palavras))\n",
        "\n",
        "# Executar a primeira etapa\n",
        "processar_curriculos()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-f9vISimIzl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from docx import Document\n",
        "\n",
        "# Diretórios\n",
        "DIR_PROC = \"/content/Processamento\"\n",
        "DIR_OUT = \"/content/Output\"\n",
        "\n",
        "# POS a serem filtradas\n",
        "FILTER_POS = {'PRON', 'DET', 'ADP', 'AUX', 'CCONJ', 'SCONJ', 'PART', 'INTJ', 'PUNCT', 'SPACE', 'NUM'}\n",
        "\n",
        "# Carregando modelo spaCy\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "def limpar_texto(texto):\n",
        "    doc = nlp(texto.lower())\n",
        "    return \" \".join([token.lemma_ for token in doc if token.pos_ not in FILTER_POS and not token.is_stop and not token.is_punct])\n",
        "\n",
        "# Função para ler arquivos DOCX do diretório\n",
        "def ler_arquivos_docx(diretorio):\n",
        "    textos = []\n",
        "    for nome_arquivo in os.listdir(diretorio):\n",
        "        if nome_arquivo.endswith(\".docx\"):\n",
        "            path = os.path.join(diretorio, nome_arquivo)\n",
        "            doc = Document(path)\n",
        "            full_text = \" \".join([p.text for p in doc.paragraphs])\n",
        "            textos.append((nome_arquivo, full_text))\n",
        "    return textos\n",
        "\n",
        "# Carregar currículos e vagas processadas\n",
        "curriculos = ler_arquivos_docx(DIR_PROC)\n",
        "vagas = ler_arquivos_docx(DIR_OUT)\n",
        "\n",
        "# Pré-processar os textos\n",
        "curriculos_proc = [(nome, limpar_texto(texto)) for nome, texto in curriculos]\n",
        "vagas_proc = [(nome, limpar_texto(texto)) for nome, texto in vagas]\n",
        "\n",
        "# Vetorizar\n",
        "all_docs = [texto for _, texto in curriculos_proc + vagas_proc]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(all_docs)\n",
        "\n",
        "# Separar vetores\n",
        "vetores_curriculos = X[:len(curriculos_proc)]\n",
        "vetores_vagas = X[len(curriculos_proc):]\n",
        "\n",
        "# Calcular similaridade\n",
        "resultado_analise = []\n",
        "for i, (nome_curriculo, _) in enumerate(curriculos_proc):\n",
        "    for j, (nome_vaga, texto_vaga) in enumerate(vagas_proc):\n",
        "        sim = cosine_similarity(vetores_curriculos[i], vetores_vagas[j])[0][0]\n",
        "        resultado_analise.append({\n",
        "            \"curriculo\": nome_curriculo,\n",
        "            \"vaga\": nome_vaga,\n",
        "            \"similaridade\": round(sim * 100, 2)\n",
        "        })\n",
        "\n",
        "# Exportar resultados\n",
        "output_doc = Document()\n",
        "output_doc.add_heading(\"Análise de Aderência entre Currículos e Vagas\", level=1)\n",
        "\n",
        "for resultado in sorted(resultado_analise, key=lambda x: x['similaridade'], reverse=True):\n",
        "    output_doc.add_paragraph(\n",
        "        f\"Currículo: {resultado['curriculo']} \\n\"\n",
        "        f\"Vaga: {resultado['vaga']} \\n\"\n",
        "        f\"Aderência: {resultado['similaridade']}%\"\n",
        "    )\n",
        "    output_doc.add_paragraph(\"\\n\")\n",
        "\n",
        "# Salvar resultado\n",
        "saida_final = os.path.join(DIR_PROC, \"resultado_aderencia.docx\")\n",
        "output_doc.save(saida_final)\n",
        "print(f\"Análise salva em: {saida_final}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "g3l_CcilgnFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from docx import Document\n",
        "\n",
        "# Diretórios\n",
        "DIR_PROC = \"/content/Processamento\"\n",
        "DIR_OUT = \"/content/Output\"\n",
        "\n",
        "# POS a serem filtradas\n",
        "FILTER_POS = {'PRON', 'DET', 'ADP', 'AUX', 'CCONJ', 'SCONJ', 'PART', 'INTJ', 'PUNCT', 'SPACE', 'NUM'}\n",
        "\n",
        "# Carregando modelo spaCy\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "def limpar_texto(texto):\n",
        "    doc = nlp(texto.lower())\n",
        "    return \" \".join([token.lemma_ for token in doc if token.pos_ not in FILTER_POS and not token.is_stop and not token.is_punct])\n",
        "\n",
        "# Função para ler arquivos DOCX do diretório\n",
        "def ler_arquivos_docx(diretorio):\n",
        "    textos = []\n",
        "    for nome_arquivo in os.listdir(diretorio):\n",
        "        if nome_arquivo.endswith(\".docx\"):\n",
        "            path = os.path.join(diretorio, nome_arquivo)\n",
        "            doc = Document(path)\n",
        "            full_text = \" \".join([p.text for p in doc.paragraphs])\n",
        "            textos.append((nome_arquivo, full_text))\n",
        "    return textos\n",
        "\n",
        "# Carregar currículos e vagas processadas\n",
        "curriculos = ler_arquivos_docx(DIR_PROC)\n",
        "vagas = ler_arquivos_docx(DIR_OUT)\n",
        "\n",
        "# Pré-processar os textos\n",
        "curriculos_proc = [(nome, limpar_texto(texto)) for nome, texto in curriculos]\n",
        "vagas_proc = [(nome, limpar_texto(texto)) for nome, texto in vagas]\n",
        "\n",
        "# Vetorizar\n",
        "all_docs = [texto for _, texto in curriculos_proc + vagas_proc]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(all_docs)\n",
        "\n",
        "# Separar vetores\n",
        "vetores_curriculos = X[:len(curriculos_proc)]\n",
        "vetores_vagas = X[len(curriculos_proc):]\n",
        "\n",
        "# Calcular similaridade\n",
        "resultado_analise = []\n",
        "for i, (nome_curriculo, _) in enumerate(curriculos_proc):\n",
        "    for j, (nome_vaga, texto_vaga) in enumerate(vagas_proc):\n",
        "        sim = cosine_similarity(vetores_curriculos[i], vetores_vagas[j])[0][0]\n",
        "        resultado_analise.append({\n",
        "            \"curriculo\": nome_curriculo,\n",
        "            \"vaga\": nome_vaga,\n",
        "            \"similaridade\": round(sim * 100, 2)\n",
        "        })\n",
        "\n",
        "# Exportar resultados\n",
        "output_doc = Document()\n",
        "output_doc.add_heading(\"Análise de Aderência entre Currículos e Vagas\", level=1)\n",
        "\n",
        "for resultado in sorted(resultado_analise, key=lambda x: x['similaridade'], reverse=True):\n",
        "    output_doc.add_paragraph(\n",
        "        f\"Currículo: {resultado['curriculo']} \\n\"\n",
        "        f\"Vaga: {resultado['vaga']} \\n\"\n",
        "        f\"Aderência: {resultado['similaridade']}%\"\n",
        "    )\n",
        "    output_doc.add_paragraph(\"\\n\")\n",
        "\n",
        "# Salvar resultado\n",
        "saida_final = os.path.join(DIR_PROC, \"resultado_aderencia.docx\")\n",
        "output_doc.save(saida_final)\n",
        "print(f\"Análise salva em: {saida_final}\")\n"
      ],
      "metadata": {
        "id": "Z1UGqqBoI1LV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLlxQ79rMLsVfHy7lE6Q/W",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}